{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries to setup the notebook.  \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import numerical libraries.  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor\n",
    "from sklearn.model_selection import ShuffleSplit,cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file I wrote to load the data from the tab-delimited text files.  \n",
    "import dream\n",
    "import loading\n",
    "import scoring\n",
    "import fit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Compound Identifier', 'Odor', 'Replicate', 'Intensity', 'Dilution', 'subject #', 'INTENSITY/STRENGTH', 'VALENCE/PLEASANTNESS', 'BAKERY', 'SWEET', 'FRUIT', 'FISH', 'GARLIC', 'SPICES', 'COLD', 'SOUR', 'BURNT', 'ACID', 'WARM', 'MUSKY', 'SWEATY', 'AMMONIA/URINOUS', 'DECAYED', 'WOOD', 'GRASS', 'FLOWER', 'CHEMICAL']\n"
     ]
    }
   ],
   "source": [
    "# Load the perceptual descriptors data.  \n",
    "perceptual_headers, perceptual_obs_data = loading.load_perceptual_data('training')\n",
    "#여기서 training 은 train_set 데이터를 가리킨다. 따라서 perceptual data는 train_set의 데이터값들을 나타낸다. \n",
    "loading.format_leaderboard_perceptual_data()\n",
    "# Show the perceptual metadata types and perceptual descriptor names.\n",
    "print(perceptual_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['126', '4-Hydroxybenzaldehyde', False, 'high', '1/10', '1', 37, 60, 0, 72, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Show the metadata and perceptual descriptor values for the first compound.\n",
    "print(perceptual_obs_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21 different perceptual descriptors and 49 different subjects\n"
     ]
    }
   ],
   "source": [
    "num_descriptors = len(perceptual_headers[6:])\n",
    "#dream.NUM_DESCRIPTORS가 무엇인지 도저히 모르겠다. 알아보자.\n",
    "#assert는 조건문 으로써 assert다음에 오는 문장이 참이 아님면 밑으로 실행이 되지 않는다. \n",
    "assert num_descriptors == dream.NUM_DESCRIPTORS\n",
    "num_subjects = dream.NUM_SUBJECTS\n",
    "print('There are %d different perceptual descriptors and %d different subjects' % (num_descriptors,num_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4870\n",
      "476\n",
      "First ten molecular descriptor types are ['CID', 'complexity from pubmed', 'MW', 'AMW', 'Sv', 'Se', 'Sp', 'Si', 'Mv', 'Me']\n",
      "First ten descriptor values for the first compound are ['126', 93.1, 122.13, 8.142, 10.01, 15.305, 10.193, 16.664, 0.667, 1.02]\n",
      "We have molecular descriptors for 476 unique molecules\n"
     ]
    }
   ],
   "source": [
    "# Load the molecular descriptors data.  \n",
    "#아래의 코드는 molecular_descriptors_data.txt텍스트를 여는 코드이다. \n",
    "molecular_headers, molecular_data = loading.load_molecular_data()\n",
    "#feature(x)의 개수 4870\n",
    "#y의 개수 476\n",
    "print(len(molecular_headers)) \n",
    "print(len(molecular_data))\n",
    "print(\"First ten molecular descriptor types are %s\" % molecular_headers[:10])\n",
    "print(\"First ten descriptor values for the first compound are %s\" % molecular_data[0][:10])\n",
    "total_size = len(set([int(row[0]) for row in molecular_data]))\n",
    "print(\"We have molecular descriptors for %d unique molecules\" % total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35084 rows in the perceptual data set (at least one for each subject and molecule)\n",
      "1960 of these are replicates (same subject and molecules)\n"
     ]
    }
   ],
   "source": [
    "#정확하게 len(perceptual_obs_data) 는 총 데이터의 수 \n",
    "print(\"There are %d rows in the perceptual data set (at least one for each subject and molecule)\" % len(perceptual_obs_data))\n",
    "print(\"%d of these are replicates (same subject and molecules)\" % sum([x[2] for x in perceptual_obs_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Molecular Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8193, 6658, 31234, 220674, 1032, 8712, 12297, 12810, 31244, 12813, 31246, 7695, 31249, 85522, 8723, 15380, 31252, 521238, 1049, 7194, 98330, 31260, 31265, 31268, 4133, 12327, 6184, 7720, 9256, 31272, 1068, 9261, 10285, 10797, 444972, 5364729, 7731, 24116, 638014, 21057, 6213, 7749, 60999, 61005, 78925, 7761, 7762, 8785, 7765, 1110, 61527, 7768, 23642, 8797, 6753, 5363388, 61027, 520296, 5362798, 1136, 7795, 62580, 17525, 7799, 7288, 61048, 18554, 7803, 10364, 61052, 126, 11902, 444539, 637563, 10882, 637566, 6276, 24197, 9862, 62087, 440967, 650, 10890, 6287, 7824, 10895, 21648, 5779, 660, 5780, 15510, 101010, 6429333, 8857, 14491, 1183, 10400, 11428, 2214, 7335, 8363, 31404, 7341, 10925, 176, 177, 14514, 3578033, 8375, 8892, 14525, 10430, 15037, 3776, 7360, 7361, 23235, 196, 62144, 2758, 93375, 5280450, 61641, 61130, 18635, 8908, 11980, 10448, 17617, 8914, 61138, 61653, 7894, 8918, 12506, 26331, 61659, 17121, 1549026, 101604, 6374, 61670, 7915, 7916, 643820, 239, 240, 241, 7410, 7921, 244, 13561, 7937, 24834, 61185, 261, 11525, 263, 6920, 8456, 11527, 11529, 61192, 62725, 444683, 61199, 5368076, 104721, 439570, 61204, 14104, 61209, 526618, 5362814, 6943, 7967, 7969, 5363491, 12580, 5365027, 22310, 807, 7463, 2345, 2346, 12587, 165675, 61229, 7983, 6448, 11569, 82227, 92979, 3893, 5315892, 311, 7991, 9016, 5365049, 10748, 7997, 5950, 2879, 11583, 9025, 12097, 27457, 27458, 325, 326, 8007, 5960, 5961, 69963, 7500, 11086, 145742, 159055, 170833, 32594, 637776, 6997, 6998, 22873, 8030, 7519, 11614, 11617, 89440, 356, 6501, 15717, 16741, 637796, 6505, 8042, 875, 61293, 19310, 61945, 61809, 62835, 519539, 9589, 887, 379, 62332, 5273467, 8063, 16255, 88454, 9609, 18827, 556940, 8077, 61325, 641423, 1551246, 8082, 7059, 12178, 6549, 12180, 14228, 61331, 2969, 61337, 8091, 235414, 8093, 5362588, 7583, 6560, 228769, 6050, 5367706, 5541, 6054, 8103, 62374, 6057, 6569, 7593, 62375, 31225, 62378, 6999977, 14257, 62900, 8118, 7095, 6584, 62902, 8122, 8635, 565690, 957, 6590, 8125, 8129, 24513, 16324, 12741, 246728, 61386, 460, 14286, 7119, 7122, 7635, 8658, 36822, 7127, 14296, 595928, 6106, 61918, 7136, 61408, 994, 5366244, 7654, 7144, 1001, 5610, 7147, 31209, 62444, 7150, 7151, 8174, 106997, 8184, 8697, 8186, 6140, 7165, 520191}\n",
      "338\n",
      "We have perceptual data for 338 unique molecules\n",
      "138 are left out for testing in the competition; half of these (69) are used for the leaderboard.\n"
     ]
    }
   ],
   "source": [
    "training_size = len(set([int(row[0]) for row in perceptual_obs_data]))\n",
    "#perceptual_data에서 하나씩 뽑아와서 row\n",
    "print(set([int(row[0]) for row in perceptual_obs_data]))\n",
    "#위의 코드는 len(set([int(row[0]) for row in perceptual_obs_data]))의 결과가 어떻게 되는지 보려고 삽입함. 좀더 알아보자. row[0]이 이해안됨\n",
    "print(training_size)\n",
    "#training_size의 크기 보려고 입력함 위에서 처럼 set으로 받으면 중복 없애 주어서 실험용 분자의 개수를 알 수 있다.\n",
    "\n",
    "print(\"We have perceptual data for %d unique molecules\" % training_size)\n",
    "remaining_size = total_size - training_size\n",
    "print (\"%d are left out for testing in the competition; half of these (%d) are used for the leaderboard.\" \\\n",
    "       % (remaining_size,remaining_size/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import sys\n",
    "#위의 sys는 필요 없어서 지움\n",
    "#밑의 print들은 원하는대로 바뀌었는지 검사하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 676 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (676x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (676x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (676x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (676x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (676x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_training,good1,good2,means,stds,imputer = dream.make_X(molecular_data,\"training\")\n",
    "#dream.make_X해석하기, 특히 assert kind in ['training','leaderboard','testset']이게 뭔지 알기. <-알아서 설명 적어둠\n",
    "X_training.shape\n",
    "#if type(\"training\") is str:\n",
    "    #training = [\"training\"]\n",
    "#rint(type(training))\n",
    "#print(\"training\")\n",
    "#print(training)\n",
    "\n",
    "#밑의 코드는 위의 결과를 csv파일로 저장하는 과정이다. \n",
    "df_X_training = pd.DataFrame(data = X_training)\n",
    "df_X_training.to_csv('결과1/X_training.csv')\n",
    "\n",
    "df_X_training_good1 = pd.DataFrame(data = good1)\n",
    "df_X_training_good1.to_csv('결과1/X_training_good1.csv')\n",
    "\n",
    "df_X_training_good2 = pd.DataFrame(data = good2)\n",
    "df_X_training_good2.to_csv('결과1/X_training_good2.csv')\n",
    "\n",
    "df_X_training_means = pd.DataFrame(data = means)\n",
    "df_X_training_means.to_csv('결과1/X_training_means.csv')\n",
    "\n",
    "df_X_training_stds = pd.DataFrame(data = stds)\n",
    "df_X_training_stds.to_csv('결과1/X_training_stds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6     \\\n",
      "0   -0.097027 -0.453737  0.581957 -0.638775 -0.920075 -0.810565 -0.998679   \n",
      "1   -0.097027 -0.453737  0.581957 -0.638775 -0.920075 -0.810565 -0.998679   \n",
      "2   -1.380812 -2.303775  0.360150 -2.486655 -2.205301 -2.673468 -2.213874   \n",
      "3   -1.380812 -2.303775  0.360150 -2.486655 -2.205301 -2.673468 -2.213874   \n",
      "4   -2.272064 -2.983815 -0.102210 -2.825315 -2.529353 -2.879935 -2.469563   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "671  0.500907  0.575051 -0.360776  0.724985  0.831187  0.752246  0.844477   \n",
      "672  0.645542  0.088929  0.158888  0.153410 -0.160561  0.099693 -0.194121   \n",
      "673  0.645542  0.088929  0.158888  0.153410 -0.160561  0.099693 -0.194121   \n",
      "674  0.018099  0.089142 -0.184785  0.040156  0.250776 -0.015968  0.244074   \n",
      "675  0.018099  0.089142 -0.184785  0.040156  0.250776 -0.015968  0.244074   \n",
      "\n",
      "         7         8         9     ...      3023      3024      3025  \\\n",
      "0    1.282713  1.231423  0.532590  ... -0.244051 -0.054473 -0.478634   \n",
      "1    1.282713  1.231423  0.532590  ... -0.244051 -0.054473 -0.478634   \n",
      "2   -0.201101  2.792361 -0.744244  ... -0.244051 -0.054473 -0.478634   \n",
      "3   -0.201101  2.792361 -0.744244  ... -0.244051 -0.054473 -0.478634   \n",
      "4   -0.529195  0.944013 -0.593371  ... -0.244051 -0.054473 -0.478634   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "671 -0.514090 -0.363556 -0.340399  ... -0.244051 -0.054473  2.089277   \n",
      "672  0.870246 -0.314705  0.523106  ... -0.244051 -0.054473 -0.478634   \n",
      "673  0.870246 -0.314705  0.523106  ... -0.244051 -0.054473 -0.478634   \n",
      "674 -0.574624  0.607264 -0.561365  ... -0.244051 -0.054473 -0.478634   \n",
      "675 -0.574624  0.607264 -0.561365  ... -0.244051 -0.054473 -0.478634   \n",
      "\n",
      "         3026      3027      3028      3029      3030  3031  3032  \n",
      "0   -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -3.0  -2.0  \n",
      "1   -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -1.0  -2.0  \n",
      "2   -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -7.0  -6.0  \n",
      "3   -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -5.0  -6.0  \n",
      "4   -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -5.0  -4.0  \n",
      "..        ...       ...       ...       ...       ...   ...   ...  \n",
      "671 -0.134433 -0.396906 -0.077152  1.459325 -0.094632  -3.0  -4.0  \n",
      "672 -0.134433 -0.396906 -0.077152  1.459325 -0.094632  -7.0  -6.0  \n",
      "673 -0.134433 -0.396906 -0.077152  1.459325 -0.094632  -5.0  -6.0  \n",
      "674 -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -5.0  -4.0  \n",
      "675 -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -3.0  -4.0  \n",
      "\n",
      "[676 rows x 3033 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_X_training)\n",
    "#X_training\n",
    "#결과는 잘 나왔다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 69 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (69x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (69x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (69x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (69x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (69x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_leaderboard_other,good1,good2,means,stds,imputer = dream.make_X(molecular_data,\"leaderboard\",target_dilution='high',good1=good1,good2=good2,means=means,stds=stds)\n",
    "X_leaderboard_other.shape\n",
    "#print(X_leaderboard_other)\n",
    "\n",
    "#print(means)\n",
    "\n",
    "#print(imputer)\n",
    "\n",
    "df_X_leaderboard_other = pd.DataFrame(data = X_leaderboard_other)\n",
    "df_X_leaderboard_other.to_csv('결과1/X_leaderboard_other.csv')\n",
    "\n",
    "df_X_leaderboard_other_good1 = pd.DataFrame(data = good1)\n",
    "df_X_leaderboard_other_good1.to_csv('결과1/X_leaderboard_other_good1.csv')\n",
    "\n",
    "df_X_leaderboard_other_good2 = pd.DataFrame(data = good2)\n",
    "df_X_leaderboard_other_good2.to_csv('결과1/X_leaderboard_other_good2.csv')\n",
    "\n",
    "df_X_leaderboard_other_means = pd.DataFrame(data = means)\n",
    "df_X_leaderboard_other_means.to_csv('결과1/X_leaderboard_other_means.csv')\n",
    "\n",
    "df_X_leaderboard_other_stds = pd.DataFrame(data = stds)\n",
    "df_X_leaderboard_other_stds.to_csv('결과1/X_leaderboard_other_stds.csv')\n",
    "\n",
    "#sys.stdout = open('결과1/X_leaderboard_other.txt', 'w')\n",
    "#print(X_leaderboard_other)\n",
    "\n",
    "#일단 이거 살피다가 데이터 전처리 하는것은 넘어가고 빠르게 결과로 가자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 69 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (69x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (69x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (69x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (69x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (69x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_leaderboard_int,good1,good2,means,stds,imputer = dream.make_X(molecular_data,\"leaderboard\",target_dilution=-3,good1=good1,good2=good2,means=means,stds=stds)\n",
    "X_leaderboard_int.shape\n",
    "\n",
    "df_X_leaderboard_int = pd.DataFrame(data = X_leaderboard_int)\n",
    "df_X_leaderboard_int.to_csv('결과1/X_leaderboard_int.csv')\n",
    "\n",
    "df_X_leaderboard_int_good1 = pd.DataFrame(data = good1)\n",
    "df_X_leaderboard_int_good1.to_csv('결과1/X_leaderboard_int_good1.csv')\n",
    "\n",
    "df_X_leaderboard_int_good2 = pd.DataFrame(data = good2)\n",
    "df_X_leaderboard_int_good2.to_csv('결과1/X_leaderboard_int_good2.csv')\n",
    "\n",
    "df_X_leaderboard_int_means = pd.DataFrame(data = means)\n",
    "df_X_leaderboard_int_means.to_csv('결과1/X_leaderboard_int_means.csv')\n",
    "\n",
    "df_X_leaderboard_int_stds = pd.DataFrame(data = stds)\n",
    "df_X_leaderboard_int_stds.to_csv('결과1/X_leaderboard_int_stds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 69 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (69x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (69x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (69x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (69x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (69x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_testset_other,good1,good2,means,stds,imputer = dream.make_X(molecular_data,\"testset\",target_dilution='high',good1=good1,good2=good2,means=means,stds=stds)\n",
    "X_testset_other.shape\n",
    "\n",
    "df_X_testset_other = pd.DataFrame(data = X_testset_other)\n",
    "df_X_testset_other.to_csv('결과1/X_testset_other.csv')\n",
    "\n",
    "df_X_testset_other_good1 = pd.DataFrame(data = good1)\n",
    "df_X_testset_other_good1.to_csv('결과1/X_testset_other_good1.csv')\n",
    "\n",
    "df_X_testset_other_good2 = pd.DataFrame(data = good2)\n",
    "df_X_testset_other_good2.to_csv('결과1/X_testset_other_good2.csv')\n",
    "\n",
    "df_X_testset_other_means = pd.DataFrame(data = means)\n",
    "df_X_testset_other_means.to_csv('결과1/X_testset_other_means.csv')\n",
    "\n",
    "df_X_testset_other_stds = pd.DataFrame(data = stds)\n",
    "df_X_testset_other_stds.to_csv('결과1/X_testset_other_stds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 69 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (69x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (69x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (69x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (69x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (69x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_testset_int,good1,good2,means,stds,imputer = dream.make_X(molecular_data,\"testset\",target_dilution=-3,good1=good1,good2=good2,means=means,stds=stds)\n",
    "X_testset_int.shape\n",
    "\n",
    "df_X_testset_int = pd.DataFrame(data = X_testset_int)\n",
    "df_X_testset_int.to_csv('결과1/X_testset_int.csv')\n",
    "\n",
    "df_X_testset_int_good1 = pd.DataFrame(data = good1)\n",
    "df_X_testset_int_good1.to_csv('결과1/X_testset_int_good1.csv')\n",
    "\n",
    "df_X_testset_int_good2 = pd.DataFrame(data = good2)\n",
    "df_X_testset_int_good2.to_csv('결과1/X_testset_int_good2.csv')\n",
    "\n",
    "df_X_testset_int_means = pd.DataFrame(data = means)\n",
    "df_X_testset_int_means.to_csv('결과1/X_testset_int_means.csv')\n",
    "\n",
    "df_X_testset_int_stds = pd.DataFrame(data = stds)\n",
    "df_X_testset_int_stds.to_csv('결과1/X_testset_int_stds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 814 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (814x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (814x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (814x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (814x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (814x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_all,good1,good2,means,stds,imputer = dream.make_X(molecular_data,[\"training\",\"leaderboard\"],good1=good1,good2=good2,means=means,stds=stds)\n",
    "X_all.shape\n",
    "\n",
    "\n",
    "df_X_all = pd.DataFrame(data = X_all)\n",
    "df_X_all.to_csv('결과1/X_all.csv')\n",
    "\n",
    "df_X_all_good1 = pd.DataFrame(data = good1)\n",
    "df_X_all_good1.to_csv('결과1/X_all_good1.csv')\n",
    "\n",
    "df_X_all_good2 = pd.DataFrame(data = good2)\n",
    "df_X_all_good2.to_csv('결과1/X_all_good2.csv')\n",
    "\n",
    "df_X_all_means = pd.DataFrame(data = means)\n",
    "df_X_all_means.to_csv('결과1/X_all_means.csv')\n",
    "\n",
    "df_X_all_stds = pd.DataFrame(data = stds)\n",
    "df_X_all_stds.to_csv('결과1/X_all_stds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting basic perceptual data...\n",
      "Flattening into vectors...\n",
      "Assembling into matrices...\n",
      "Y_obs['subject'] contains 49 matrices each with shape (676x21) (molecules by perceptual descriptors)\n",
      "The Y_obs['mean_std'] matrix has shape (676x42) (molecules by 2 x perceptual descriptors)\n",
      "Combining Y matrices...\n",
      "The Y['mean_std'] matrix now has shape (676x42) molecules by 2 x perceptual descriptors\n",
      "The Y['subject'] dict now has 49 matrices of shape (676x21) molecules by perceptual descriptors, one for each subject\n"
     ]
    }
   ],
   "source": [
    "Y_training,imputer = dream.make_Y_obs('training',target_dilution=None,imputer='median')\n",
    "\n",
    "#df_Y_training_subject = pd.DataFrame(data = Y_training['subject'])\n",
    "#df_Y_training_subject.to_csv('결과1/Y_training_subject.csv')\n",
    "\n",
    "#df_Y_training_mean_std = pd.DataFrame(data = Y_training['mean_std'])\n",
    "#df_Y_training_mean_std.to_csv('결과1/Y_training_means_std.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subject', 'mean_std'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_training['subject'])\n",
    "#y의 출력결과도 알기 위해서 type을 알아봄. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training['subject'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ma.core.MaskedArray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_training['subject'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한사람이 몇개 검사했는지 데이터 보기 위해서 따로 하나 뽑음\n",
    "\n",
    "df_Y_training_subject_1 = pd.DataFrame(data = Y_training['subject'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y_training_subject_1.to_csv('결과1/Y_training_subject_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range(2,50) 2~49까지 for문 돌린다. \n",
    "for i in range(1, 50):\n",
    "    df_Y_training_subject = pd.DataFrame(data = Y_training['subject'][i])\n",
    "    df_Y_training_subject.to_csv('결과1/Y_training_subject.csv')\n",
    "\n",
    "##이렇게 하면 한 파일에 다 저장이 되는데 어떻게 하면 48개의 파일로 만들 수 있을지 생각해보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_training['mean_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24.65306122, 51.05882353,  0.41176471, ...,  4.92349559,\n",
       "        14.12498915, 18.87043024],\n",
       "       [49.55102041, 48.95652174,  0.63043478, ...,  6.35156605,\n",
       "        22.14199891, 21.47108985],\n",
       "       [ 4.55102041, 47.25      ,  0.5625    , ...,  7.40178751,\n",
       "        10.87374274, 15.80739377],\n",
       "       ...,\n",
       "       [12.85714286, 36.5483871 ,  1.35483871, ..., 12.34315578,\n",
       "        20.18113539, 23.60691192],\n",
       "       [17.83673469, 45.28571429,  3.9047619 , ...,  7.18795288,\n",
       "        13.90835937, 14.4307322 ],\n",
       "       [45.14285714, 51.19047619, 13.66666667, ...,  7.45093075,\n",
       "        13.60462471, 22.31703608]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training['mean_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y_training_mean_std = pd.DataFrame(data = Y_training['mean_std'])\n",
    "df_Y_training_mean_std.to_csv('결과1/Y_training_means_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting basic perceptual data...\n",
      "Flattening into vectors...\n",
      "Assembling into matrices...\n",
      "Y_obs['subject'] contains 49 matrices each with shape (69x21) (molecules by perceptual descriptors)\n",
      "The Y_obs['mean_std'] matrix has shape (69x42) (molecules by 2 x perceptual descriptors)\n",
      "Combining Y matrices...\n",
      "The Y['mean_std'] matrix now has shape (69x42) molecules by 2 x perceptual descriptors\n",
      "The Y['subject'] dict now has 49 matrices of shape (69x21) molecules by perceptual descriptors, one for each subject\n",
      "Getting basic perceptual data...\n",
      "Flattening into vectors...\n",
      "Assembling into matrices...\n",
      "Y_obs['subject'] contains 49 matrices each with shape (69x21) (molecules by perceptual descriptors)\n",
      "The Y_obs['mean_std'] matrix has shape (69x42) (molecules by 2 x perceptual descriptors)\n",
      "Combining Y matrices...\n",
      "The Y['mean_std'] matrix now has shape (69x42) molecules by 2 x perceptual descriptors\n",
      "The Y['subject'] dict now has 49 matrices of shape (69x21) molecules by perceptual descriptors, one for each subject\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['subject', 'mean_std'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_leaderboard,imputer = dream.make_Y_obs('leaderboard',target_dilution='gold',imputer='mask')\n",
    "\n",
    "Y_leaderboard.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 50):\n",
    "    df_Y_leaderboard_subject = pd.DataFrame(data = Y_leaderboard['subject'][i])\n",
    "    df_Y_leaderboard_subject.to_csv('결과1/Y_leaderboard_subject.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y_leaderboard_mean_std = pd.DataFrame(data = Y_leaderboard['mean_std'])\n",
    "df_Y_leaderboard_mean_std.to_csv('결과1/Y_leaderboard_means_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting basic perceptual data...\n",
      "Flattening into vectors...\n",
      "Assembling into matrices...\n",
      "Y_obs['subject'] contains 49 matrices each with shape (676x21) (molecules by perceptual descriptors)\n",
      "The Y_obs['mean_std'] matrix has shape (676x42) (molecules by 2 x perceptual descriptors)\n",
      "Getting basic perceptual data...\n",
      "Flattening into vectors...\n",
      "Assembling into matrices...\n",
      "Y_obs['subject'] contains 49 matrices each with shape (138x21) (molecules by perceptual descriptors)\n",
      "The Y_obs['mean_std'] matrix has shape (138x42) (molecules by 2 x perceptual descriptors)\n",
      "Combining Y matrices...\n",
      "The Y['mean_std'] matrix now has shape (814x42) molecules by 2 x perceptual descriptors\n",
      "The Y['subject'] dict now has 49 matrices of shape (814x21) molecules by perceptual descriptors, one for each subject\n"
     ]
    }
   ],
   "source": [
    "Y_all_imp,imputer = dream.make_Y_obs(['training','leaderboard'],target_dilution=None,imputer='median')\n",
    "\n",
    "for i in range(1, 50):\n",
    "    df_Y_all_imp_subject = pd.DataFrame(data = Y_all_imp['subject'][i])\n",
    "    df_Y_all_imp_subject.to_csv('결과1/Y_all_imp_subject.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y_all_imp_mean_std = pd.DataFrame(data = Y_all_imp['mean_std'])\n",
    "df_Y_all_imp_mean_std.to_csv('결과1/Y_all_imp_means_std.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Perceptual descriptor values')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the range of values for the molecular and perceptual descriptors.  \n",
    "plt.hist(X_training.ravel())\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Cube root transformed, N(0,1) normalized molecular descriptor values')\n",
    "plt.figure()\n",
    "plt.hist(np.dstack([Y_training['subject'][subject] for subject in range(1,50)]).ravel())\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Perceptual descriptor values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Generating Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한번 write를 true로 바꾸어 보도록 하자. \n",
    "write = True # Set to True to actually generate the prediction files.  \n",
    "#write = False\n",
    "n_estimators = 5 # Set this to a high number (e.g. 100) to get a good fit.  \n",
    "\n",
    "# Best parameters, determined independently.  \n",
    "max_features = {'int':None,\n",
    "                'ple':100,\n",
    "                'dec':500}\n",
    "min_samples_leaf = {'int':1,\n",
    "                'ple':1,\n",
    "                'dec':1}\n",
    "max_depth = {'int':None,\n",
    "                'ple':10,\n",
    "                'dec':10}\n",
    "et = {'int':True,\n",
    "      'ple':False,\n",
    "      'dec':False,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For subchallenge 1:\n",
      "\tScore = 27.27\n",
      "\tint = 0.887\n",
      "\tple = 0.141\n",
      "\tdec = 0.111\n"
     ]
    }
   ],
   "source": [
    "# Fit training data.  \n",
    "rfcs_leaderboard,score,rs = fit1.rfc_final(X_training,Y_training['subject'],\n",
    "                            max_features,min_samples_leaf,max_depth,et,\n",
    "                            n_estimators=n_estimators)\n",
    "\n",
    "#df_rfcs_leaderboard = pd.DataFrame(data = rfcs)\n",
    "#df_rfcs_leaderboard.to_csv('결과1/rfcs_leaderboard.csv')\n",
    "\n",
    "#밑의 코드를 실행하면 DataFrame constructor not properly called!의 오류가 뜬다. \n",
    "#df_rfcs_leaderboard_score = pd.DataFrame(data = score)\n",
    "#df_rfcs_leaderboard_score.to_csv('결과1/rfcs_leaderboard_score.csv')\n",
    "\n",
    "#df_rfcs_leaderboard_rs= pd.DataFrame(data = rs)\n",
    "#df_rfcs_leaderboard_rs.to_csv('결과1/rfcs_leaderboard_rs.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#밑의 과정들은 저장을 위해서 결과의 data tpye들을 알아보는 중이다. \n",
    "type(rfcs_leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['int', 'ple', 'dec'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfcs_leaderboard.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': {1: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  2: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  3: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  4: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  5: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  6: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  7: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  8: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  9: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  10: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  11: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  12: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  13: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  14: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  15: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  16: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  17: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  18: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  19: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  20: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  21: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  22: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  23: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  24: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  25: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  26: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  27: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  28: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  29: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  30: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  31: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  32: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  33: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  34: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  35: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  36: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  37: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  38: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  39: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  40: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  41: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  42: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  43: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  44: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  45: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  46: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  47: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  48: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  49: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False)},\n",
       " 'ple': {1: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  2: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  3: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  4: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  5: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  6: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  7: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  8: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  9: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  10: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  11: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  12: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  13: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  14: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  15: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  16: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  17: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  18: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  19: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  20: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  21: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  22: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  23: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  24: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  25: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  26: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  27: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  28: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  29: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  30: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  31: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  32: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  33: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  34: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  35: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  36: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  37: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  38: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  39: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  40: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  41: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  42: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  43: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  44: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  45: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  46: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  47: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  48: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  49: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False)},\n",
       " 'dec': {1: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  2: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  3: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  4: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  5: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  6: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  7: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  8: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  9: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  10: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  11: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  12: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  13: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  14: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  15: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  16: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  17: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  18: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  19: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  20: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  21: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  22: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  23: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  24: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  25: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  26: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  27: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  28: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  29: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  30: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  31: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  32: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  33: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  34: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  35: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  36: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  37: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  38: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  39: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  40: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  41: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  42: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  43: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  44: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  45: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  46: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  47: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  48: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  49: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False)}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfcs_leaderboard = pd.DataFrame(data = rfcs_leaderboard)\n",
    "df_rfcs_leaderboard.to_csv('결과1/rfcs_leaderboard.csv')\n",
    "rfcs_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': 0.8865399027875027,\n",
       " 'ple': 0.14068000604003744,\n",
       " 'dec': 0.11088710246673102}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/Desktop/git/dream-master practice/scoring.py:103: UserWarning: Warning: converting a masked element to nan.\n",
      "  if ('%f' % r_) == 'nan':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 9.856691; rs = 0.434,0.029,0.020\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'submissions/challenge_1_leaderboard_1583819985.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-6f6f09430833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_prediction_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfcs_leaderboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_leaderboard_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_leaderboard_other\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'leaderboard'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_leaderboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Print values for subject 1 to make sure it looks right.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df_rfcs_leaderboard_predictionresult = pd.DataFrame(data = _)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#df_rfcs_leaderboard.to_csv('결과1/rfcs_leaderboard.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/dream-master practice/loading.py\u001b[0m in \u001b[0;36mmake_prediction_files\u001b[0;34m(rfcs, X_int, X_other, target, subchallenge, Y_test, write, trans_weight, regularize, name)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mwrite_prediction_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubchallenge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Wrote to file with suffix \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/dream-master practice/loading.py\u001b[0m in \u001b[0;36mwrite_prediction_files\u001b[0;34m(Y, kind, subchallenge, name)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_prediction_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubchallenge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_prediction_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubchallenge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m     \u001b[0mCIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_CIDs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mperceptual_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_perceptual_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/git/dream-master practice/loading.py\u001b[0m in \u001b[0;36mopen_prediction_file\u001b[0;34m(subchallenge, kind, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;31m# Write predictions for each subchallenge to a file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_prediction_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubchallenge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submissions/challenge_%d_%s_%s.txt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubchallenge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'submissions/challenge_1_leaderboard_1583819985.txt'"
     ]
    }
   ],
   "source": [
    "_ = loading.make_prediction_files(rfcs_leaderboard,X_leaderboard_int,X_leaderboard_other,'leaderboard',1,Y_test=Y_leaderboard,write=write)\n",
    "print(_['subject'][1]) # Print values for subject 1 to make sure it looks right.  \n",
    "\n",
    "#df_rfcs_leaderboard_predictionresult = pd.DataFrame(data = _)\n",
    "#df_rfcs_leaderboard.to_csv('결과1/rfcs_leaderboard.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfcs_leaderboard_predictionresult = pd.DataFrame(data = _)\n",
    "df_rfcs_leaderboard_predictionresult.to_csv('결과1/rfcs_leaderboard_predictionresult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcs,score,rs = fit1.rfc_final(X_all,Y_all_imp['subject'],\n",
    "                            max_features,min_samples_leaf,max_depth,et,\n",
    "                            n_estimators=n_estimators)\n",
    "\n",
    "df_rfcs = pd.DataFrame(data = rfcs)\n",
    "df_rfcs.to_csv('결과1/rfcs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = loading.make_prediction_files(rfcs,X_testset_int,X_testset_other,'testset',1,write=False)\n",
    "print(_['subject'][1]) # Print values for subject 1 to make sure it looks right. \n",
    "\n",
    "df_rfcs_predictionresult = pd.DataFrame(data = _)\n",
    "df_rfcs_predictionresult.to_csv('결과1/rfcs_predictionresult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('py36': conda)",
   "language": "python",
   "name": "python361064bitpy36condafebe6ef44cbe421b8ee9be5fd6c2df2e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
