{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries to setup the notebook.  \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import numerical libraries.  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor\n",
    "from sklearn.model_selection import ShuffleSplit,cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file I wrote to load the data from the tab-delimited text files.  \n",
    "import dream\n",
    "import loading\n",
    "import scoring\n",
    "import fit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Compound Identifier', 'Odor', 'Replicate', 'Intensity', 'Dilution', 'subject #', 'INTENSITY/STRENGTH', 'VALENCE/PLEASANTNESS', 'BAKERY', 'SWEET', 'FRUIT', 'FISH', 'GARLIC', 'SPICES', 'COLD', 'SOUR', 'BURNT', 'ACID', 'WARM', 'MUSKY', 'SWEATY', 'AMMONIA/URINOUS', 'DECAYED', 'WOOD', 'GRASS', 'FLOWER', 'CHEMICAL']\n"
     ]
    }
   ],
   "source": [
    "# Load the perceptual descriptors data.  \n",
    "perceptual_headers, perceptual_obs_data = loading.load_perceptual_data('training')\n",
    "#여기서 training 은 train_set 데이터를 가리킨다. 따라서 perceptual data는 train_set의 데이터값들을 나타낸다. \n",
    "loading.format_leaderboard_perceptual_data()\n",
    "# Show the perceptual metadata types and perceptual descriptor names.\n",
    "print(perceptual_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['126', '4-Hydroxybenzaldehyde', False, 'high', '1/10', '1', 37, 60, 0, 72, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Show the metadata and perceptual descriptor values for the first compound.\n",
    "print(perceptual_obs_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21 different perceptual descriptors and 49 different subjects\n"
     ]
    }
   ],
   "source": [
    "num_descriptors = len(perceptual_headers[6:])\n",
    "#dream.NUM_DESCRIPTORS가 무엇인지 도저히 모르겠다. 알아보자.\n",
    "#assert는 조건문 으로써 assert다음에 오는 문장이 참이 아님면 밑으로 실행이 되지 않는다. \n",
    "assert num_descriptors == dream.NUM_DESCRIPTORS\n",
    "num_subjects = dream.NUM_SUBJECTS\n",
    "print('There are %d different perceptual descriptors and %d different subjects' % (num_descriptors,num_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4870\n",
      "476\n",
      "First ten molecular descriptor types are ['CID', 'complexity from pubmed', 'MW', 'AMW', 'Sv', 'Se', 'Sp', 'Si', 'Mv', 'Me']\n",
      "First ten descriptor values for the first compound are ['126', 93.1, 122.13, 8.142, 10.01, 15.305, 10.193, 16.664, 0.667, 1.02]\n",
      "We have molecular descriptors for 476 unique molecules\n"
     ]
    }
   ],
   "source": [
    "# Load the molecular descriptors data.  \n",
    "#아래의 코드는 molecular_descriptors_data.txt텍스트를 여는 코드이다. \n",
    "molecular_headers, molecular_data = loading.load_molecular_data()\n",
    "#feature(x)의 개수 4870\n",
    "#y의 개수 476\n",
    "print(len(molecular_headers)) \n",
    "print(len(molecular_data))\n",
    "print(\"First ten molecular descriptor types are %s\" % molecular_headers[:10])\n",
    "print(\"First ten descriptor values for the first compound are %s\" % molecular_data[0][:10])\n",
    "total_size = len(set([int(row[0]) for row in molecular_data]))\n",
    "print(\"We have molecular descriptors for %d unique molecules\" % total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35084 rows in the perceptual data set (at least one for each subject and molecule)\n",
      "1960 of these are replicates (same subject and molecules)\n"
     ]
    }
   ],
   "source": [
    "#정확하게 len(perceptual_obs_data) 는 총 데이터의 수 \n",
    "print(\"There are %d rows in the perceptual data set (at least one for each subject and molecule)\" % len(perceptual_obs_data))\n",
    "print(\"%d of these are replicates (same subject and molecules)\" % sum([x[2] for x in perceptual_obs_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Molecular Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8193, 6658, 31234, 220674, 1032, 8712, 12297, 12810, 31244, 12813, 31246, 7695, 31249, 85522, 8723, 15380, 31252, 521238, 1049, 7194, 98330, 31260, 31265, 31268, 4133, 12327, 6184, 7720, 9256, 31272, 1068, 9261, 10285, 10797, 444972, 5364729, 7731, 24116, 638014, 21057, 6213, 7749, 60999, 61005, 78925, 7761, 7762, 8785, 7765, 1110, 61527, 7768, 23642, 8797, 6753, 5363388, 61027, 520296, 5362798, 1136, 7795, 62580, 17525, 7799, 7288, 61048, 18554, 7803, 10364, 61052, 126, 11902, 444539, 637563, 10882, 637566, 6276, 24197, 9862, 62087, 440967, 650, 10890, 6287, 7824, 10895, 21648, 5779, 660, 5780, 15510, 101010, 6429333, 8857, 14491, 1183, 10400, 11428, 2214, 7335, 8363, 31404, 7341, 10925, 176, 177, 14514, 3578033, 8375, 8892, 14525, 10430, 15037, 3776, 7360, 7361, 23235, 196, 62144, 2758, 93375, 5280450, 61641, 61130, 18635, 8908, 11980, 10448, 17617, 8914, 61138, 61653, 7894, 8918, 12506, 26331, 61659, 17121, 1549026, 101604, 6374, 61670, 7915, 7916, 643820, 239, 240, 241, 7410, 7921, 244, 13561, 7937, 24834, 61185, 261, 11525, 263, 6920, 8456, 11527, 11529, 61192, 62725, 444683, 61199, 5368076, 104721, 439570, 61204, 14104, 61209, 526618, 5362814, 6943, 7967, 7969, 5363491, 12580, 5365027, 22310, 807, 7463, 2345, 2346, 12587, 165675, 61229, 7983, 6448, 11569, 82227, 92979, 3893, 5315892, 311, 7991, 9016, 5365049, 10748, 7997, 5950, 2879, 11583, 9025, 12097, 27457, 27458, 325, 326, 8007, 5960, 5961, 69963, 7500, 11086, 145742, 159055, 170833, 32594, 637776, 6997, 6998, 22873, 8030, 7519, 11614, 11617, 89440, 356, 6501, 15717, 16741, 637796, 6505, 8042, 875, 61293, 19310, 61945, 61809, 62835, 519539, 9589, 887, 379, 62332, 5273467, 8063, 16255, 88454, 9609, 18827, 556940, 8077, 61325, 641423, 1551246, 8082, 7059, 12178, 6549, 12180, 14228, 61331, 2969, 61337, 8091, 235414, 8093, 5362588, 7583, 6560, 228769, 6050, 5367706, 5541, 6054, 8103, 62374, 6057, 6569, 7593, 62375, 31225, 62378, 6999977, 14257, 62900, 8118, 7095, 6584, 62902, 8122, 8635, 565690, 957, 6590, 8125, 8129, 24513, 16324, 12741, 246728, 61386, 460, 14286, 7119, 7122, 7635, 8658, 36822, 7127, 14296, 595928, 6106, 61918, 7136, 61408, 994, 5366244, 7654, 7144, 1001, 5610, 7147, 31209, 62444, 7150, 7151, 8174, 106997, 8184, 8697, 8186, 6140, 7165, 520191}\n",
      "338\n",
      "We have perceptual data for 338 unique molecules\n",
      "138 are left out for testing in the competition; half of these (69) are used for the leaderboard.\n"
     ]
    }
   ],
   "source": [
    "training_size = len(set([int(row[0]) for row in perceptual_obs_data]))\n",
    "#perceptual_data에서 하나씩 뽑아와서 row\n",
    "print(set([int(row[0]) for row in perceptual_obs_data]))\n",
    "#위의 코드는 len(set([int(row[0]) for row in perceptual_obs_data]))의 결과가 어떻게 되는지 보려고 삽입함. 좀더 알아보자. row[0]이 이해안됨\n",
    "print(training_size)\n",
    "#training_size의 크기 보려고 입력함 위에서 처럼 set으로 받으면 중복 없애 주어서 실험용 분자의 개수를 알 수 있다.\n",
    "\n",
    "print(\"We have perceptual data for %d unique molecules\" % training_size)\n",
    "remaining_size = total_size - training_size\n",
    "print (\"%d are left out for testing in the competition; half of these (%d) are used for the leaderboard.\" \\\n",
    "       % (remaining_size,remaining_size/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import sys\n",
    "#위의 sys는 필요 없어서 지움\n",
    "#밑의 print들은 원하는대로 바뀌었는지 검사하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 676 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (676x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (676x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (676x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (676x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (676x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_training,good1,good2,means,stds,imputer = dream.make_X(molecular_data,\"training\")\n",
    "#dream.make_X해석하기, 특히 assert kind in ['training','leaderboard','testset']이게 뭔지 알기. <-알아서 설명 적어둠\n",
    "X_training.shape\n",
    "#if type(\"training\") is str:\n",
    "    #training = [\"training\"]\n",
    "#rint(type(training))\n",
    "#print(\"training\")\n",
    "#print(training)\n",
    "\n",
    "#밑의 코드는 위의 결과를 csv파일로 저장하는 과정이다. \n",
    "df_X_training = pd.DataFrame(data = X_training)\n",
    "df_X_training.to_csv('결과1/X_training.csv')\n",
    "\n",
    "df_X_training_good1 = pd.DataFrame(data = good1)\n",
    "df_X_training_good1.to_csv('결과1/X_training_good1.csv')\n",
    "\n",
    "df_X_training_good2 = pd.DataFrame(data = good2)\n",
    "df_X_training_good2.to_csv('결과1/X_training_good2.csv')\n",
    "\n",
    "df_X_training_means = pd.DataFrame(data = means)\n",
    "df_X_training_means.to_csv('결과1/X_training_means.csv')\n",
    "\n",
    "df_X_training_stds = pd.DataFrame(data = stds)\n",
    "df_X_training_stds.to_csv('결과1/X_training_stds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6     \\\n",
      "0   -0.097027 -0.453737  0.581957 -0.638775 -0.920075 -0.810565 -0.998679   \n",
      "1   -0.097027 -0.453737  0.581957 -0.638775 -0.920075 -0.810565 -0.998679   \n",
      "2   -1.380812 -2.303775  0.360150 -2.486655 -2.205301 -2.673468 -2.213874   \n",
      "3   -1.380812 -2.303775  0.360150 -2.486655 -2.205301 -2.673468 -2.213874   \n",
      "4   -2.272064 -2.983815 -0.102210 -2.825315 -2.529353 -2.879935 -2.469563   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "671  0.500907  0.575051 -0.360776  0.724985  0.831187  0.752246  0.844477   \n",
      "672  0.645542  0.088929  0.158888  0.153410 -0.160561  0.099693 -0.194121   \n",
      "673  0.645542  0.088929  0.158888  0.153410 -0.160561  0.099693 -0.194121   \n",
      "674  0.018099  0.089142 -0.184785  0.040156  0.250776 -0.015968  0.244074   \n",
      "675  0.018099  0.089142 -0.184785  0.040156  0.250776 -0.015968  0.244074   \n",
      "\n",
      "         7         8         9     ...      3023      3024      3025  \\\n",
      "0    1.282713  1.231423  0.532590  ... -0.244051 -0.054473 -0.478634   \n",
      "1    1.282713  1.231423  0.532590  ... -0.244051 -0.054473 -0.478634   \n",
      "2   -0.201101  2.792361 -0.744244  ... -0.244051 -0.054473 -0.478634   \n",
      "3   -0.201101  2.792361 -0.744244  ... -0.244051 -0.054473 -0.478634   \n",
      "4   -0.529195  0.944013 -0.593371  ... -0.244051 -0.054473 -0.478634   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "671 -0.514090 -0.363556 -0.340399  ... -0.244051 -0.054473  2.089277   \n",
      "672  0.870246 -0.314705  0.523106  ... -0.244051 -0.054473 -0.478634   \n",
      "673  0.870246 -0.314705  0.523106  ... -0.244051 -0.054473 -0.478634   \n",
      "674 -0.574624  0.607264 -0.561365  ... -0.244051 -0.054473 -0.478634   \n",
      "675 -0.574624  0.607264 -0.561365  ... -0.244051 -0.054473 -0.478634   \n",
      "\n",
      "         3026      3027      3028      3029      3030  3031  3032  \n",
      "0   -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -3.0  -2.0  \n",
      "1   -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -1.0  -2.0  \n",
      "2   -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -7.0  -6.0  \n",
      "3   -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -5.0  -6.0  \n",
      "4   -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -5.0  -4.0  \n",
      "..        ...       ...       ...       ...       ...   ...   ...  \n",
      "671 -0.134433 -0.396906 -0.077152  1.459325 -0.094632  -3.0  -4.0  \n",
      "672 -0.134433 -0.396906 -0.077152  1.459325 -0.094632  -7.0  -6.0  \n",
      "673 -0.134433 -0.396906 -0.077152  1.459325 -0.094632  -5.0  -6.0  \n",
      "674 -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -5.0  -4.0  \n",
      "675 -0.134433 -0.396906 -0.077152 -0.685248 -0.094632  -3.0  -4.0  \n",
      "\n",
      "[676 rows x 3033 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_X_training)\n",
    "#X_training\n",
    "#결과는 잘 나왔다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 69 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (69x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (69x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (69x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (69x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (69x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_leaderboard_other,good1,good2,means,stds,imputer = dream.make_X(molecular_data,\"leaderboard\",target_dilution='high',good1=good1,good2=good2,means=means,stds=stds)\n",
    "X_leaderboard_other.shape\n",
    "#print(X_leaderboard_other)\n",
    "\n",
    "#print(means)\n",
    "\n",
    "#print(imputer)\n",
    "\n",
    "df_X_leaderboard_other = pd.DataFrame(data = X_leaderboard_other)\n",
    "df_X_leaderboard_other.to_csv('결과1/X_leaderboard_other.csv')\n",
    "\n",
    "df_X_leaderboard_other_good1 = pd.DataFrame(data = good1)\n",
    "df_X_leaderboard_other_good1.to_csv('결과1/X_leaderboard_other_good1.csv')\n",
    "\n",
    "df_X_leaderboard_other_good2 = pd.DataFrame(data = good2)\n",
    "df_X_leaderboard_other_good2.to_csv('결과1/X_leaderboard_other_good2.csv')\n",
    "\n",
    "df_X_leaderboard_other_means = pd.DataFrame(data = means)\n",
    "df_X_leaderboard_other_means.to_csv('결과1/X_leaderboard_other_means.csv')\n",
    "\n",
    "df_X_leaderboard_other_stds = pd.DataFrame(data = stds)\n",
    "df_X_leaderboard_other_stds.to_csv('결과1/X_leaderboard_other_stds.csv')\n",
    "\n",
    "#sys.stdout = open('결과1/X_leaderboard_other.txt', 'w')\n",
    "#print(X_leaderboard_other)\n",
    "\n",
    "#일단 이거 살피다가 데이터 전처리 하는것은 넘어가고 빠르게 결과로 가자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 69 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (69x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (69x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (69x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (69x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (69x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_leaderboard_int,good1,good2,means,stds,imputer = dream.make_X(molecular_data,\"leaderboard\",target_dilution=-3,good1=good1,good2=good2,means=means,stds=stds)\n",
    "X_leaderboard_int.shape\n",
    "\n",
    "df_X_leaderboard_int = pd.DataFrame(data = X_leaderboard_int)\n",
    "df_X_leaderboard_int.to_csv('결과1/X_leaderboard_int.csv')\n",
    "\n",
    "df_X_leaderboard_int_good1 = pd.DataFrame(data = good1)\n",
    "df_X_leaderboard_int_good1.to_csv('결과1/X_leaderboard_int_good1.csv')\n",
    "\n",
    "df_X_leaderboard_int_good2 = pd.DataFrame(data = good2)\n",
    "df_X_leaderboard_int_good2.to_csv('결과1/X_leaderboard_int_good2.csv')\n",
    "\n",
    "df_X_leaderboard_int_means = pd.DataFrame(data = means)\n",
    "df_X_leaderboard_int_means.to_csv('결과1/X_leaderboard_int_means.csv')\n",
    "\n",
    "df_X_leaderboard_int_stds = pd.DataFrame(data = stds)\n",
    "df_X_leaderboard_int_stds.to_csv('결과1/X_leaderboard_int_stds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 69 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (69x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (69x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (69x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (69x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (69x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_testset_other,good1,good2,means,stds,imputer = dream.make_X(molecular_data,\"testset\",target_dilution='high',good1=good1,good2=good2,means=means,stds=stds)\n",
    "X_testset_other.shape\n",
    "\n",
    "df_X_testset_other = pd.DataFrame(data = X_testset_other)\n",
    "df_X_testset_other.to_csv('결과1/X_testset_other.csv')\n",
    "\n",
    "df_X_testset_other_good1 = pd.DataFrame(data = good1)\n",
    "df_X_testset_other_good1.to_csv('결과1/X_testset_other_good1.csv')\n",
    "\n",
    "df_X_testset_other_good2 = pd.DataFrame(data = good2)\n",
    "df_X_testset_other_good2.to_csv('결과1/X_testset_other_good2.csv')\n",
    "\n",
    "df_X_testset_other_means = pd.DataFrame(data = means)\n",
    "df_X_testset_other_means.to_csv('결과1/X_testset_other_means.csv')\n",
    "\n",
    "df_X_testset_other_stds = pd.DataFrame(data = stds)\n",
    "df_X_testset_other_stds.to_csv('결과1/X_testset_other_stds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 69 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (69x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (69x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (69x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (69x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (69x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_testset_int,good1,good2,means,stds,imputer = dream.make_X(molecular_data,\"testset\",target_dilution=-3,good1=good1,good2=good2,means=means,stds=stds)\n",
    "X_testset_int.shape\n",
    "\n",
    "df_X_testset_int = pd.DataFrame(data = X_testset_int)\n",
    "df_X_testset_int.to_csv('결과1/X_testset_int.csv')\n",
    "\n",
    "df_X_testset_int_good1 = pd.DataFrame(data = good1)\n",
    "df_X_testset_int_good1.to_csv('결과1/X_testset_int_good1.csv')\n",
    "\n",
    "df_X_testset_int_good2 = pd.DataFrame(data = good2)\n",
    "df_X_testset_int_good2.to_csv('결과1/X_testset_int_good2.csv')\n",
    "\n",
    "df_X_testset_int_means = pd.DataFrame(data = means)\n",
    "df_X_testset_int_means.to_csv('결과1/X_testset_int_means.csv')\n",
    "\n",
    "df_X_testset_int_stds = pd.DataFrame(data = stds)\n",
    "df_X_testset_int_stds.to_csv('결과1/X_testset_int_stds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CIDs and dilutions...\n",
      "Getting basic molecular data...\n",
      "Adding dilution data...\n",
      "There are now 814 molecular vectors of length 4871, one for each molecule and dilution\n",
      "Building a matrix...\n",
      "The X matrix has shape (814x4871) (molecules by molecular descriptors)\n",
      "Purging data with too many NaNs...\n",
      "The X matrix has shape (814x4871) (molecules by good molecular descriptors)\n",
      "Imputing remaining NaN data...\n",
      "The X matrix now has shape (814x4871) (molecules by non-NaN good molecular descriptors)\n",
      "Purging data that is still bad, if any...\n",
      "The X matrix has shape (814x3033) (molecules by good molecular descriptors)\n",
      "Normalizing data for fitting...\n",
      "The X matrix now has shape (814x3033) molecules by non-NaN good molecular descriptors\n"
     ]
    }
   ],
   "source": [
    "X_all,good1,good2,means,stds,imputer = dream.make_X(molecular_data,[\"training\",\"leaderboard\"],good1=good1,good2=good2,means=means,stds=stds)\n",
    "X_all.shape\n",
    "\n",
    "\n",
    "df_X_all = pd.DataFrame(data = X_all)\n",
    "df_X_all.to_csv('결과1/X_all.csv')\n",
    "\n",
    "df_X_all_good1 = pd.DataFrame(data = good1)\n",
    "df_X_all_good1.to_csv('결과1/X_all_good1.csv')\n",
    "\n",
    "df_X_all_good2 = pd.DataFrame(data = good2)\n",
    "df_X_all_good2.to_csv('결과1/X_all_good2.csv')\n",
    "\n",
    "df_X_all_means = pd.DataFrame(data = means)\n",
    "df_X_all_means.to_csv('결과1/X_all_means.csv')\n",
    "\n",
    "df_X_all_stds = pd.DataFrame(data = stds)\n",
    "df_X_all_stds.to_csv('결과1/X_all_stds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting basic perceptual data...\n",
      "Flattening into vectors...\n",
      "Assembling into matrices...\n",
      "Y_obs['subject'] contains 49 matrices each with shape (676x21) (molecules by perceptual descriptors)\n",
      "The Y_obs['mean_std'] matrix has shape (676x42) (molecules by 2 x perceptual descriptors)\n",
      "Combining Y matrices...\n",
      "The Y['mean_std'] matrix now has shape (676x42) molecules by 2 x perceptual descriptors\n",
      "The Y['subject'] dict now has 49 matrices of shape (676x21) molecules by perceptual descriptors, one for each subject\n"
     ]
    }
   ],
   "source": [
    "Y_training,imputer = dream.make_Y_obs('training',target_dilution=None,imputer='median')\n",
    "\n",
    "#df_Y_training_subject = pd.DataFrame(data = Y_training['subject'])\n",
    "#df_Y_training_subject.to_csv('결과1/Y_training_subject.csv')\n",
    "\n",
    "#df_Y_training_mean_std = pd.DataFrame(data = Y_training['mean_std'])\n",
    "#df_Y_training_mean_std.to_csv('결과1/Y_training_means_std.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subject', 'mean_std'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_training['subject'])\n",
    "#y의 출력결과도 알기 위해서 type을 알아봄. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training['subject'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ma.core.MaskedArray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_training['subject'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한사람이 몇개 검사했는지 데이터 보기 위해서 따로 하나 뽑음\n",
    "\n",
    "df_Y_training_subject_1 = pd.DataFrame(data = Y_training['subject'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y_training_subject_1.to_csv('결과1/Y_training_subject_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range(2,50) 2~49까지 for문 돌린다. \n",
    "for i in range(1, 50):\n",
    "    df_Y_training_subject = pd.DataFrame(data = Y_training['subject'][i])\n",
    "    df_Y_training_subject.to_csv('결과1/Y_training_subject.csv')\n",
    "\n",
    "##이렇게 하면 한 파일에 다 저장이 되는데 어떻게 하면 48개의 파일로 만들 수 있을지 생각해보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_training['mean_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24.65306122, 51.05882353,  0.41176471, ...,  4.92349559,\n",
       "        14.12498915, 18.87043024],\n",
       "       [49.55102041, 48.95652174,  0.63043478, ...,  6.35156605,\n",
       "        22.14199891, 21.47108985],\n",
       "       [ 4.55102041, 47.25      ,  0.5625    , ...,  7.40178751,\n",
       "        10.87374274, 15.80739377],\n",
       "       ...,\n",
       "       [12.85714286, 36.5483871 ,  1.35483871, ..., 12.34315578,\n",
       "        20.18113539, 23.60691192],\n",
       "       [17.83673469, 45.28571429,  3.9047619 , ...,  7.18795288,\n",
       "        13.90835937, 14.4307322 ],\n",
       "       [45.14285714, 51.19047619, 13.66666667, ...,  7.45093075,\n",
       "        13.60462471, 22.31703608]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training['mean_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y_training_mean_std = pd.DataFrame(data = Y_training['mean_std'])\n",
    "df_Y_training_mean_std.to_csv('결과1/Y_training_means_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting basic perceptual data...\n",
      "Flattening into vectors...\n",
      "Assembling into matrices...\n",
      "Y_obs['subject'] contains 49 matrices each with shape (69x21) (molecules by perceptual descriptors)\n",
      "The Y_obs['mean_std'] matrix has shape (69x42) (molecules by 2 x perceptual descriptors)\n",
      "Combining Y matrices...\n",
      "The Y['mean_std'] matrix now has shape (69x42) molecules by 2 x perceptual descriptors\n",
      "The Y['subject'] dict now has 49 matrices of shape (69x21) molecules by perceptual descriptors, one for each subject\n",
      "Getting basic perceptual data...\n",
      "Flattening into vectors...\n",
      "Assembling into matrices...\n",
      "Y_obs['subject'] contains 49 matrices each with shape (69x21) (molecules by perceptual descriptors)\n",
      "The Y_obs['mean_std'] matrix has shape (69x42) (molecules by 2 x perceptual descriptors)\n",
      "Combining Y matrices...\n",
      "The Y['mean_std'] matrix now has shape (69x42) molecules by 2 x perceptual descriptors\n",
      "The Y['subject'] dict now has 49 matrices of shape (69x21) molecules by perceptual descriptors, one for each subject\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['subject', 'mean_std'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_leaderboard,imputer = dream.make_Y_obs('leaderboard',target_dilution='gold',imputer='mask')\n",
    "\n",
    "Y_leaderboard.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 50):\n",
    "    df_Y_leaderboard_subject = pd.DataFrame(data = Y_leaderboard['subject'][i])\n",
    "    df_Y_leaderboard_subject.to_csv('결과1/Y_leaderboard_subject.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y_leaderboard_mean_std = pd.DataFrame(data = Y_leaderboard['mean_std'])\n",
    "df_Y_leaderboard_mean_std.to_csv('결과1/Y_leaderboard_means_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting basic perceptual data...\n",
      "Flattening into vectors...\n",
      "Assembling into matrices...\n",
      "Y_obs['subject'] contains 49 matrices each with shape (676x21) (molecules by perceptual descriptors)\n",
      "The Y_obs['mean_std'] matrix has shape (676x42) (molecules by 2 x perceptual descriptors)\n",
      "Getting basic perceptual data...\n",
      "Flattening into vectors...\n",
      "Assembling into matrices...\n",
      "Y_obs['subject'] contains 49 matrices each with shape (138x21) (molecules by perceptual descriptors)\n",
      "The Y_obs['mean_std'] matrix has shape (138x42) (molecules by 2 x perceptual descriptors)\n",
      "Combining Y matrices...\n",
      "The Y['mean_std'] matrix now has shape (814x42) molecules by 2 x perceptual descriptors\n",
      "The Y['subject'] dict now has 49 matrices of shape (814x21) molecules by perceptual descriptors, one for each subject\n"
     ]
    }
   ],
   "source": [
    "Y_all_imp,imputer = dream.make_Y_obs(['training','leaderboard'],target_dilution=None,imputer='median')\n",
    "\n",
    "for i in range(1, 50):\n",
    "    df_Y_all_imp_subject = pd.DataFrame(data = Y_all_imp['subject'][i])\n",
    "    df_Y_all_imp_subject.to_csv('결과1/Y_all_imp_subject.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y_all_imp_mean_std = pd.DataFrame(data = Y_all_imp['mean_std'])\n",
    "df_Y_all_imp_mean_std.to_csv('결과1/Y_all_imp_means_std.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Perceptual descriptor values')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the range of values for the molecular and perceptual descriptors.  \n",
    "plt.hist(X_training.ravel())\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Cube root transformed, N(0,1) normalized molecular descriptor values')\n",
    "plt.figure()\n",
    "plt.hist(np.dstack([Y_training['subject'][subject] for subject in range(1,50)]).ravel())\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Perceptual descriptor values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Generating Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한번 write를 true로 바꾸어 보도록 하자. \n",
    "write = True # Set to True to actually generate the prediction files.  \n",
    "#write = False\n",
    "n_estimators = 5 # Set this to a high number (e.g. 100) to get a good fit.  \n",
    "\n",
    "# Best parameters, determined independently.  \n",
    "max_features = {'int':None,\n",
    "                'ple':100,\n",
    "                'dec':500}\n",
    "min_samples_leaf = {'int':1,\n",
    "                'ple':1,\n",
    "                'dec':1}\n",
    "max_depth = {'int':None,\n",
    "                'ple':10,\n",
    "                'dec':10}\n",
    "et = {'int':True,\n",
    "      'ple':False,\n",
    "      'dec':False,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For subchallenge 1:\n",
      "\tScore = 27.27\n",
      "\tint = 0.887\n",
      "\tple = 0.141\n",
      "\tdec = 0.111\n"
     ]
    }
   ],
   "source": [
    "# Fit training data.  \n",
    "rfcs_leaderboard,score,rs = fit1.rfc_final(X_training,Y_training['subject'],\n",
    "                            max_features,min_samples_leaf,max_depth,et,\n",
    "                            n_estimators=n_estimators)\n",
    "\n",
    "#df_rfcs_leaderboard = pd.DataFrame(data = rfcs)\n",
    "#df_rfcs_leaderboard.to_csv('결과1/rfcs_leaderboard.csv')\n",
    "\n",
    "#밑의 코드를 실행하면 DataFrame constructor not properly called!의 오류가 뜬다. \n",
    "#df_rfcs_leaderboard_score = pd.DataFrame(data = score)\n",
    "#df_rfcs_leaderboard_score.to_csv('결과1/rfcs_leaderboard_score.csv')\n",
    "\n",
    "#df_rfcs_leaderboard_rs= pd.DataFrame(data = rs)\n",
    "#df_rfcs_leaderboard_rs.to_csv('결과1/rfcs_leaderboard_rs.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#밑의 과정들은 저장을 위해서 결과의 data tpye들을 알아보는 중이다. \n",
    "type(rfcs_leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['int', 'ple', 'dec'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfcs_leaderboard.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': {1: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  2: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  3: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  4: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  5: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  6: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  7: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  8: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  9: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  10: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  11: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  12: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  13: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  14: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  15: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  16: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  17: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  18: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  19: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  20: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  21: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  22: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  23: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  24: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  25: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  26: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  27: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  28: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  29: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  30: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  31: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  32: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  33: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  34: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  35: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  36: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  37: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  38: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  39: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  40: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  41: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  42: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  43: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  44: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  45: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  46: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  47: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  48: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False),\n",
       "  49: ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=5, n_jobs=-1, oob_score=False, random_state=0,\n",
       "                      verbose=0, warm_start=False)},\n",
       " 'ple': {1: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  2: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  3: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  4: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  5: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  6: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  7: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  8: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  9: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  10: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  11: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  12: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  13: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  14: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  15: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  16: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  17: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  18: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  19: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  20: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  21: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  22: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  23: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  24: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  25: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  26: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  27: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  28: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  29: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  30: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  31: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  32: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  33: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  34: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  35: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  36: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  37: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  38: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  39: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  40: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  41: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  42: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  43: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  44: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  45: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  46: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  47: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  48: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  49: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=100, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False)},\n",
       " 'dec': {1: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  2: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  3: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  4: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  5: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  6: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  7: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  8: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  9: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  10: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  11: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  12: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  13: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  14: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  15: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  16: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  17: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  18: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  19: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  20: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  21: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  22: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  23: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  24: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  25: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  26: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  27: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  28: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  29: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  30: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  31: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  32: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  33: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  34: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  35: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  36: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  37: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  38: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  39: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  40: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  41: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  42: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  43: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  44: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  45: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  46: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  47: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  48: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False),\n",
       "  49: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                        max_depth=10, max_features=500, max_leaf_nodes=None,\n",
       "                        max_samples=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=5, n_jobs=-1, oob_score=True, random_state=0,\n",
       "                        verbose=0, warm_start=False)}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rfcs_leaderboard = pd.DataFrame(data = rfcs_leaderboard)\n",
    "df_rfcs_leaderboard.to_csv('결과1/rfcs_leaderboard.csv')\n",
    "rfcs_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int': 0.8865399027875027,\n",
       " 'ple': 0.14068000604003744,\n",
       " 'dec': 0.11088710246673102}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/Desktop/git/dream-master practice/scoring.py:103: UserWarning: Warning: converting a masked element to nan.\n",
      "  if ('%f' % r_) == 'nan':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 9.856691; rs = 0.434,0.029,0.020\n",
      "Wrote to file with suffix \"1583821099\"\n",
      "[[67.74       45.53485926 11.38694673 ...  1.3139134   3.64005314\n",
      "  16.30871753]\n",
      " [64.04       47.1324885   1.33120084 ...  2.77306025  4.29559165\n",
      "  21.32311689]\n",
      " [37.12285714 34.38721598  2.43839143 ...  2.00697227  2.37423025\n",
      "  11.38369428]\n",
      " ...\n",
      " [27.21428571 48.51978481  1.66148507 ...  2.81808713  5.63339503\n",
      "   9.04406456]\n",
      " [52.84857143 40.75642332  2.11523345 ...  2.09871171  6.20992644\n",
      "  15.88222253]\n",
      " [46.59142857 46.1390201   2.12696753 ...  3.41152515  6.13856617\n",
      "  12.15222307]]\n"
     ]
    }
   ],
   "source": [
    "_ = loading.make_prediction_files(rfcs_leaderboard,X_leaderboard_int,X_leaderboard_other,'leaderboard',1,Y_test=Y_leaderboard,write=write)\n",
    "print(_['subject'][1]) # Print values for subject 1 to make sure it looks right.  \n",
    "\n",
    "#df_rfcs_leaderboard_predictionresult = pd.DataFrame(data = _)\n",
    "#df_rfcs_leaderboard.to_csv('결과1/rfcs_leaderboard.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': {1: array([[67.74      , 45.53485926, 11.38694673, ...,  1.3139134 ,\n",
       "           3.64005314, 16.30871753],\n",
       "         [64.04      , 47.1324885 ,  1.33120084, ...,  2.77306025,\n",
       "           4.29559165, 21.32311689],\n",
       "         [37.12285714, 34.38721598,  2.43839143, ...,  2.00697227,\n",
       "           2.37423025, 11.38369428],\n",
       "         ...,\n",
       "         [27.21428571, 48.51978481,  1.66148507, ...,  2.81808713,\n",
       "           5.63339503,  9.04406456],\n",
       "         [52.84857143, 40.75642332,  2.11523345, ...,  2.09871171,\n",
       "           6.20992644, 15.88222253],\n",
       "         [46.59142857, 46.1390201 ,  2.12696753, ...,  3.41152515,\n",
       "           6.13856617, 12.15222307]]),\n",
       "  2: array([[67.08      , 46.93771599,  3.88408271, ...,  1.42137776,\n",
       "           3.95226188, 19.08263383],\n",
       "         [67.94      , 49.41407982,  1.37075356, ...,  3.45456862,\n",
       "           4.88936661, 14.74113049],\n",
       "         [44.92285714, 36.62494997,  2.51082138, ...,  2.1350915 ,\n",
       "           2.59358259,  9.09961521],\n",
       "         ...,\n",
       "         [28.29428571, 49.68303707,  1.57944676, ...,  3.15502335,\n",
       "           6.37446603, 11.46471299],\n",
       "         [45.76857143, 42.62532076,  2.15550283, ...,  2.43564793,\n",
       "           7.13099744, 14.12594788],\n",
       "         [43.59142857, 53.5989514 ,  2.56333816, ...,  3.81705549,\n",
       "           6.36740657, 13.58801232]]),\n",
       "  3: array([[65.22      , 42.88881743,  3.05576329, ...,  1.86904973,\n",
       "           7.43982493, 16.79669949],\n",
       "         [53.24      , 51.65037702,  1.56134536, ...,  4.43885462,\n",
       "           7.58807418, 15.02953914],\n",
       "         [45.10285714, 36.22962523,  4.57905168, ...,  2.43883566,\n",
       "           3.04648879, 14.12779843],\n",
       "         ...,\n",
       "         [24.87428571, 49.71174824,  1.9257904 , ...,  3.63062449,\n",
       "           9.49378875, 12.1959776 ],\n",
       "         [41.38857143, 47.3662856 ,  2.35586223, ...,  2.67398748,\n",
       "           8.19713804, 16.68919018],\n",
       "         [30.99142857, 51.10030989,  2.50205256, ...,  3.58829525,\n",
       "          13.28655735, 12.63460449]]),\n",
       "  4: array([[68.52      , 45.68292852,  2.75748959, ...,  1.31529995,\n",
       "          12.22070688, 33.10875941],\n",
       "         [72.2       , 49.76855236,  1.14745255, ...,  2.77306025,\n",
       "           7.84396375, 22.62215979],\n",
       "         [46.12285714, 33.91343071,  3.26696286, ...,  2.00697227,\n",
       "          14.95930961, 11.87166691],\n",
       "         ...,\n",
       "         [33.21428571, 56.92636259,  1.9988038 , ...,  4.14668477,\n",
       "          23.49803225, 11.90175673],\n",
       "         [48.94857143, 50.94869438,  1.98878717, ...,  2.05210014,\n",
       "          21.1039308 , 18.93488847],\n",
       "         [36.81142857, 49.51969467,  2.16853361, ...,  3.95688542,\n",
       "          16.48047506, 23.73569147]]),\n",
       "  5: array([[62.4       , 44.00768024,  3.19402054, ...,  2.00401696,\n",
       "           4.4680804 , 20.4889335 ],\n",
       "         [66.26      , 49.60578437,  1.57408917, ...,  3.23953281,\n",
       "           4.75391818, 14.53116661],\n",
       "         [26.80285714, 34.98962174,  2.75917155, ...,  2.34902723,\n",
       "           2.70813303,  9.0689225 ],\n",
       "         ...,\n",
       "         [18.15428571, 46.91472167,  2.10938845, ...,  3.46820945,\n",
       "           6.75739086, 11.26184026],\n",
       "         [46.66857143, 40.76081205,  2.23078717, ...,  2.16010014,\n",
       "           6.8819308 , 15.08555513],\n",
       "         [37.59142857, 48.13827948,  2.04099454, ...,  3.44490606,\n",
       "           5.90837073, 11.24623394]]),\n",
       "  6: array([[48.54      , 43.18413279,  3.1724068 , ...,  1.75341457,\n",
       "           3.96798263, 16.20324569],\n",
       "         [51.2       , 47.83594091,  1.80751726, ...,  3.67452587,\n",
       "           5.82111308, 15.00467644],\n",
       "         [37.12285714, 39.239817  ,  2.66703239, ...,  2.33733285,\n",
       "           2.99159877, 12.54114434],\n",
       "         ...,\n",
       "         [18.45428571, 45.61535717,  2.07727141, ...,  3.66171335,\n",
       "           7.28403643, 11.49764562],\n",
       "         [41.68857143, 42.30297395,  2.51357827, ...,  2.76147274,\n",
       "           7.47772587, 13.33593454],\n",
       "         [35.85142857, 48.38209281,  2.49188339, ...,  4.26385622,\n",
       "           7.1389567 , 13.09818383]]),\n",
       "  7: array([[56.82      , 48.3497164 ,  2.94395005, ...,  1.36309382,\n",
       "           4.00701225, 21.26558329],\n",
       "         [65.96      , 55.24860925,  1.19777202, ...,  2.90586512,\n",
       "           4.53504852, 16.9626467 ],\n",
       "         [43.36285714, 27.55637854,  2.43839143, ...,  4.88697227,\n",
       "           2.37423025,  7.60722247],\n",
       "         ...,\n",
       "         [37.95428571, 41.62077229,  1.48147893, ...,  2.96491264,\n",
       "           5.88449748, 14.4889223 ],\n",
       "         [57.94857143, 36.89638486,  1.98878717, ...,  2.05210014,\n",
       "           6.0339308 , 13.40155513],\n",
       "         [54.93142857, 56.53005728,  1.96254632, ...,  3.47276182,\n",
       "           6.42612166, 12.16747651]]),\n",
       "  8: array([[61.8       , 44.88433103,  3.304832  , ...,  1.8470183 ,\n",
       "           4.1581166 , 16.6726175 ],\n",
       "         [68.9       , 47.43340036,  1.56298256, ...,  3.10624342,\n",
       "           4.77357345, 13.58132435],\n",
       "         [44.86285714, 35.19297754,  2.53765802, ...,  2.07462101,\n",
       "           2.46202631,  7.99385006],\n",
       "         ...,\n",
       "         [30.39428571, 51.55323562,  1.90013345, ...,  3.30140267,\n",
       "           6.28830302, 10.47670927],\n",
       "         [43.24857143, 48.03027244,  8.25340734, ...,  2.35861955,\n",
       "           6.4767209 , 11.98866939],\n",
       "         [44.97142857, 49.54878685,  1.95540494, ...,  3.62503661,\n",
       "           5.82600628, 13.33258155]]),\n",
       "  9: array([[58.74      , 49.24754306,  3.02406109, ...,  2.58571776,\n",
       "           4.65250435, 22.43395751],\n",
       "         [63.26      , 43.3146392 ,  1.42953479, ...,  4.75371779,\n",
       "           6.86534038, 16.49548052],\n",
       "         [50.80285714, 33.78703267,  3.56437819, ...,  2.77316181,\n",
       "           3.61980749,  8.9819789 ],\n",
       "         ...,\n",
       "         [21.75428571, 50.2748329 ,  1.74089103, ...,  6.5001753 ,\n",
       "          11.94891163, 10.35133828],\n",
       "         [43.06857143, 45.63815436,  2.30001445, ...,  2.68958839,\n",
       "          11.44433854, 21.36169234],\n",
       "         [37.17142857, 49.18333521,  2.02447739, ...,  9.87131199,\n",
       "           6.5115869 , 11.60319764]]),\n",
       "  10: array([[49.08      , 46.97963869,  2.96022201, ...,  3.18907852,\n",
       "           3.94609733, 21.48277484],\n",
       "         [50.36      , 47.06727088,  1.5895161 , ...,  3.67803426,\n",
       "           5.31823531, 16.3420185 ],\n",
       "         [41.86285714, 31.12814027,  2.81126861, ...,  3.23683253,\n",
       "           3.36064846,  9.81771162],\n",
       "         ...,\n",
       "         [18.27428571, 46.96523552,  2.04674655, ...,  3.76718407,\n",
       "           7.10863454, 12.13803993],\n",
       "         [36.76857143, 45.22330022,  2.31064113, ...,  3.97980756,\n",
       "           8.18197941, 14.75960363],\n",
       "         [41.85142857, 49.51306397,  2.45536358, ...,  4.3606221 ,\n",
       "           7.0694116 , 13.55336849]]),\n",
       "  11: array([[62.1       , 42.93472949,  3.09939991, ...,  1.6367122 ,\n",
       "           4.32433432, 25.6059629 ],\n",
       "         [63.92      , 50.78523589,  2.5892883 , ...,  3.2880066 ,\n",
       "           5.30916133, 13.24201981],\n",
       "         [38.62285714, 34.26528709,  3.1989956 , ...,  3.49725914,\n",
       "           3.4955595 , 10.59643675],\n",
       "         ...,\n",
       "         [18.93428571, 47.52880212,  1.76933028, ...,  5.40127934,\n",
       "           7.48815256, 11.72122143],\n",
       "         [59.92857143, 45.32244447,  1.98878717, ...,  2.05210014,\n",
       "           6.0339308 , 17.06155513],\n",
       "         [51.09142857, 53.05772655,  2.0289556 , ...,  8.51810014,\n",
       "           6.84636807, 13.39243861]]),\n",
       "  12: array([[68.52      , 44.50847644,  3.16102297, ...,  1.57691174,\n",
       "           4.02340133, 22.03620692],\n",
       "         [65.96      , 44.21051489,  1.72223144, ...,  3.16374112,\n",
       "           5.04797626, 13.44671348],\n",
       "         [50.86285714, 30.99563563,  2.43839143, ...,  2.00697227,\n",
       "           2.37423025, 11.95085883],\n",
       "         ...,\n",
       "         [24.81428571, 45.65266886,  2.08005098, ...,  3.78452867,\n",
       "           6.72006628, 10.69365215],\n",
       "         [46.18857143, 40.15684621,  3.18434251, ...,  2.63004878,\n",
       "           6.75518767, 13.78830651],\n",
       "         [44.97142857, 48.46524174,  2.42498296, ...,  3.90474214,\n",
       "           6.9740898 , 11.89879459]]),\n",
       "  13: array([[65.28      , 47.83798223,  9.19737866, ...,  2.60697907,\n",
       "           7.7540643 , 22.71151706],\n",
       "         [66.56      , 44.25634475,  2.63471455, ...,  6.95337518,\n",
       "           6.71309417, 15.10705088],\n",
       "         [39.34285714, 37.28410749,  4.7256623 , ...,  2.75351921,\n",
       "           3.21091296,  8.87339546],\n",
       "         ...,\n",
       "         [39.51428571, 49.96137374,  2.79099522, ...,  4.690073  ,\n",
       "           7.70056998, 14.21208886],\n",
       "         [52.72857143, 43.72838624,  4.28843208, ...,  2.52672319,\n",
       "           8.98540409, 19.20519569],\n",
       "         [50.67142857, 50.36896572,  2.61368579, ...,  5.59189193,\n",
       "          13.60728468, 20.03410768]]),\n",
       "  14: array([[58.98      , 40.1683091 ,  3.18052933, ...,  2.14360493,\n",
       "           4.51629471, 16.40449787],\n",
       "         [47.6       , 48.2960349 ,  1.66814569, ...,  3.48894904,\n",
       "           5.56000224, 15.29645468],\n",
       "         [40.90285714, 38.98373356,  2.98269473, ...,  2.5536385 ,\n",
       "           3.14361913,  9.84815895],\n",
       "         ...,\n",
       "         [18.15428571, 46.96470246,  1.98673811, ...,  3.51593791,\n",
       "           6.96114183, 11.19602614],\n",
       "         [42.58857143, 47.31169586,  2.02086076, ...,  2.42517265,\n",
       "          12.73126251, 13.44493771],\n",
       "         [32.25142857, 48.55490127,  2.24490157, ...,  4.15647304,\n",
       "           6.7930848 , 12.75942271]]),\n",
       "  15: array([[60.72      , 42.09545421,  6.81633764, ...,  2.13936794,\n",
       "           4.96519859, 24.82658116],\n",
       "         [57.74      , 47.82721979,  1.52168396, ...,  3.16851555,\n",
       "           4.40924215, 25.84768517],\n",
       "         [26.92285714, 40.79713126,  3.32610415, ...,  3.39973618,\n",
       "           4.06776013, 12.51213986],\n",
       "         ...,\n",
       "         [20.61428571, 44.85645853,  2.48414623, ...,  3.40004648,\n",
       "           8.72070432, 14.20506215],\n",
       "         [41.14857143, 41.73165074,  2.30533535, ...,  2.47809105,\n",
       "           7.6511569 , 14.95704176],\n",
       "         [30.57142857, 50.57071179,  3.08984378, ...,  4.36620234,\n",
       "           8.24891424, 14.68629426]]),\n",
       "  16: array([[64.2       , 40.61985595,  3.20868007, ...,  4.2539134 ,\n",
       "           4.72071981, 24.10211753],\n",
       "         [67.94      , 44.43592536,  1.33440874, ...,  3.81701732,\n",
       "           5.71075969, 20.37859711],\n",
       "         [45.28285714, 36.12172416,  2.52771809, ...,  2.14432935,\n",
       "           2.62216043, 18.67519808],\n",
       "         ...,\n",
       "         [18.15428571, 43.25755532,  1.64906603, ...,  6.14445382,\n",
       "           7.63773549, 15.57623323],\n",
       "         [51.70857143, 46.35173398,  2.45964673, ...,  3.187348  ,\n",
       "           7.63215105, 13.52520282],\n",
       "         [40.89142857, 46.40087476,  1.96350589, ...,  6.94807701,\n",
       "           8.20135816, 15.09728286]]),\n",
       "  17: array([[59.04      , 44.2267399 ,  2.93214507, ...,  1.51743757,\n",
       "           3.94240965, 23.25956719],\n",
       "         [48.32      , 46.21186701,  1.29155198, ...,  3.23114855,\n",
       "           4.67089872, 17.76926787],\n",
       "         [27.58285714, 34.3828384 ,  3.34699263, ...,  2.87593689,\n",
       "           3.30531149, 10.34881323],\n",
       "         ...,\n",
       "         [18.15428571, 46.66595982,  1.99594283, ...,  3.44630852,\n",
       "           6.5034719 , 12.61759509],\n",
       "         [50.56857143, 39.362362  ,  2.54958324, ...,  2.66759449,\n",
       "           7.04642753, 15.55436605],\n",
       "         [40.59142857, 51.26462426,  2.24882843, ...,  3.90326174,\n",
       "           6.33209904, 19.49502056]]),\n",
       "  18: array([[65.94      , 37.83636976,  3.25000061, ...,  1.40029638,\n",
       "           3.74304347, 20.69346637],\n",
       "         [70.16      , 50.48036729,  1.14745255, ...,  2.77306025,\n",
       "           9.61396375, 17.41815979],\n",
       "         [37.48285714, 37.12499095,  2.5850581 , ...,  2.34297227,\n",
       "           2.47423025, 10.13122247],\n",
       "         ...,\n",
       "         [23.25428571, 47.7611505 ,  1.53798359, ...,  3.36569566,\n",
       "           7.15883034, 10.54487461],\n",
       "         [62.38857143, 39.84585852,  2.02199664, ...,  2.21263709,\n",
       "           6.27446415, 11.78519406],\n",
       "         [37.95142857, 49.27301651,  1.87658593, ...,  4.86596325,\n",
       "           6.5957215 , 19.53503465]]),\n",
       "  19: array([[62.4       , 38.79228543,  3.11173943, ...,  1.77056548,\n",
       "           4.40696519, 16.89738089],\n",
       "         [64.76      , 43.7497249 ,  1.50803581, ...,  3.07181267,\n",
       "           4.68327358, 15.96947448],\n",
       "         [46.36285714, 37.48675501,  2.61907032, ...,  2.16066273,\n",
       "           2.53480025,  8.02828966],\n",
       "         ...,\n",
       "         [33.27428571, 45.49304173,  1.74952059, ...,  3.20093748,\n",
       "           6.45204836, 11.35280886],\n",
       "         [47.80857143, 41.80552013,  2.32385862, ...,  2.40976757,\n",
       "           8.72461421, 13.39450491],\n",
       "         [39.99142857, 44.97718623,  2.03507138, ...,  3.60163931,\n",
       "           5.79841943, 12.57947021]]),\n",
       "  20: array([[67.62      , 40.96708546,  2.81125582, ...,  1.88865734,\n",
       "           3.93811867, 21.3637352 ],\n",
       "         [64.52      , 48.54911097,  1.17917729, ...,  3.99337267,\n",
       "           4.87792861, 18.22879355],\n",
       "         [37.18285714, 35.98734128,  2.46639526, ...,  4.27128864,\n",
       "           4.54118427,  8.65294825],\n",
       "         ...,\n",
       "         [32.91428571, 47.09398178,  1.54502853, ...,  3.14521877,\n",
       "           5.69596919, 18.15651608],\n",
       "         [45.76857143, 45.7601582 ,  2.09177006, ...,  2.35797222,\n",
       "           6.14110518, 12.44348557],\n",
       "         [33.21142857, 50.26842234,  1.96007026, ...,  3.99181042,\n",
       "           6.23583483, 13.27176568]]),\n",
       "  21: array([[62.16      , 34.16707176,  2.8017194 , ...,  1.50118669,\n",
       "           3.93685438, 22.23759579],\n",
       "         [63.98      , 39.59837523,  1.34430721, ...,  2.80513963,\n",
       "           4.38714172, 15.37746235],\n",
       "         [44.80285714, 39.54137854,  2.49724857, ...,  2.50725297,\n",
       "           2.67783175, 10.41401027],\n",
       "         ...,\n",
       "         [32.79428571, 46.22997838,  1.7772799 , ...,  3.75513696,\n",
       "           6.54600987, 10.36303335],\n",
       "         [50.86857143, 39.83529767,  2.10878717, ...,  2.29210014,\n",
       "           6.2739308 , 11.06155513],\n",
       "         [35.49142857, 47.36319752,  2.12846517, ...,  4.26455425,\n",
       "           6.88543134, 12.23144097]]),\n",
       "  22: array([[67.08      , 42.82165685,  5.56294595, ...,  1.94366383,\n",
       "           4.31118995, 19.39353823],\n",
       "         [69.5       , 43.98849667,  2.65805366, ...,  3.21994319,\n",
       "           4.75867346, 15.16497391],\n",
       "         [44.74285714, 28.44642516, 17.92836193, ...,  2.11534542,\n",
       "           2.65886051,  7.91617355],\n",
       "         ...,\n",
       "         [37.17428571, 46.13574252,  3.36249564, ...,  3.09874205,\n",
       "           6.35006632, 10.65625225],\n",
       "         [58.66857143, 36.3357524 ,  3.89687467, ...,  2.52646278,\n",
       "           6.71557585, 11.76415384],\n",
       "         [39.63142857, 48.52319455,  8.8152679 , ...,  3.50507633,\n",
       "           5.94633448, 17.51408433]]),\n",
       "  23: array([[69.9       , 42.57203135,  2.89012538, ...,  1.70575631,\n",
       "           4.487234  , 19.59808738],\n",
       "         [67.52      , 49.34662031,  1.56812123, ...,  3.20210499,\n",
       "           5.03313267, 14.51044006],\n",
       "         [28.06285714, 35.25105644,  3.04186236, ...,  2.51177274,\n",
       "           3.10514525,  9.54341964],\n",
       "         ...,\n",
       "         [19.35428571, 44.19975406,  2.15433737, ...,  3.9893083 ,\n",
       "           6.73374161,  9.70714037],\n",
       "         [36.58857143, 43.59884895,  2.27554508, ...,  2.37245382,\n",
       "           6.5388292 , 13.70025134],\n",
       "         [28.89142857, 51.80069439,  2.58718036, ...,  3.66144714,\n",
       "           6.0391966 , 10.77115162]]),\n",
       "  24: array([[61.62      , 41.6864794 ,  2.88586847, ...,  1.44355888,\n",
       "           3.73828725, 26.54645532],\n",
       "         [65.6       , 48.14462172,  1.31659632, ...,  3.74107307,\n",
       "           4.50478197, 17.71930265],\n",
       "         [35.56285714, 35.11637854,  2.44628617, ...,  2.55013017,\n",
       "           2.42580919,  7.6782751 ],\n",
       "         ...,\n",
       "         [28.89428571, 45.49298349,  1.43942017, ...,  3.52955801,\n",
       "           5.85029119,  9.86319773],\n",
       "         [48.52857143, 41.30195196,  1.98878717, ...,  2.05210014,\n",
       "           8.6379308 , 13.41355513],\n",
       "         [47.67142857, 48.27052084,  2.02614844, ...,  4.08182589,\n",
       "           6.03250304, 11.83144981]]),\n",
       "  25: array([[71.46      , 40.92852593,  5.37455885, ...,  3.76373158,\n",
       "           5.7756289 , 33.98572359],\n",
       "         [70.04      , 44.65675272,  1.33072178, ..., 11.55950255,\n",
       "           4.40944451, 27.92575594],\n",
       "         [38.32285714, 37.18899392,  2.72399143, ..., 17.33097227,\n",
       "           2.44983025, 26.76042247],\n",
       "         ...,\n",
       "         [39.33428571, 44.11139047,  4.0427311 , ..., 10.68729374,\n",
       "          15.60794485, 27.21741116],\n",
       "         [45.76857143, 42.22988486,  4.70878717, ..., 18.09391832,\n",
       "          12.12847626, 17.37064604],\n",
       "         [54.03142857, 45.25897807,  7.76797484, ...,  5.60117807,\n",
       "          12.33538536, 26.30304738]]),\n",
       "  26: array([[65.94      , 45.63734645,  9.36496937, ...,  1.49730963,\n",
       "           3.72461918, 17.99157036],\n",
       "         [68.72      , 48.94138423,  1.24899101, ..., 13.56752178,\n",
       "          11.96673298, 17.64123671],\n",
       "         [42.40285714, 38.84304521,  3.85392628, ...,  2.28953784,\n",
       "           2.5313731 ,  9.22075421],\n",
       "         ...,\n",
       "         [29.13428571, 48.86464831,  3.6517613 , ...,  3.1969806 ,\n",
       "           5.86310917, 16.98674577],\n",
       "         [51.70857143, 46.24588449,  2.95095383, ...,  2.37253764,\n",
       "           6.05369018, 27.44251049],\n",
       "         [41.67142857, 53.30900526,  4.16262281, ...,  4.11241989,\n",
       "           6.04161193, 13.62376245]]),\n",
       "  27: array([[62.46      , 45.50958412,  2.86898558, ...,  1.44005734,\n",
       "           4.07653211, 29.32982522],\n",
       "         [55.16      , 49.06090355,  1.32984837, ...,  3.1481819 ,\n",
       "           8.18099415, 22.77415943],\n",
       "         [40.90285714, 35.47556404,  2.71570609, ...,  2.24332058,\n",
       "           2.711574  , 13.87816745],\n",
       "         ...,\n",
       "         [18.39428571, 47.41533939,  1.64486491, ...,  3.13900613,\n",
       "           6.31687713, 16.92411575],\n",
       "         [42.52857143, 43.36363456,  2.62929575, ...,  2.7877577 ,\n",
       "          10.87534305, 17.66657368],\n",
       "         [39.75142857, 49.18997825,  2.08954884, ...,  3.47856751,\n",
       "           9.15813423, 23.73041228]]),\n",
       "  28: array([[64.14      , 47.41252978,  2.77493857, ...,  1.93370932,\n",
       "          12.80160416, 27.05650528],\n",
       "         [64.46      , 46.15624529,  1.70725333, ...,  4.53674659,\n",
       "           9.35893899, 16.20549168],\n",
       "         [35.02285714, 38.22583837,  4.03113736, ...,  3.8545647 ,\n",
       "           6.35451857, 16.59751121],\n",
       "         ...,\n",
       "         [21.63428571, 45.30981886,  1.57970062, ...,  3.16684475,\n",
       "          11.12994208,  9.10413256],\n",
       "         [41.44857143, 43.24821695,  2.76626951, ...,  3.3842001 ,\n",
       "           9.73079656, 12.2444136 ],\n",
       "         [36.39142857, 48.07252035,  2.98740715, ..., 13.58685091,\n",
       "           6.35434052, 11.73457272]]),\n",
       "  29: array([[67.32      , 48.93215605,  2.87319197, ...,  1.53072244,\n",
       "          15.08782754, 15.29401152],\n",
       "         [56.72      , 42.91698041,  1.20834949, ...,  3.23437754,\n",
       "          10.20837183, 13.40932507],\n",
       "         [32.80285714, 34.21363598,  2.4822128 , ...,  2.11672735,\n",
       "           2.7778724 , 15.60696396],\n",
       "         ...,\n",
       "         [18.63428571, 46.39174615,  1.56185538, ...,  4.31676396,\n",
       "           6.92538637, 10.2038823 ],\n",
       "         [55.48857143, 40.77929111,  2.0897487 , ...,  2.74707064,\n",
       "           6.92243551, 24.12518279],\n",
       "         [34.11142857, 50.54432228,  2.08042023, ...,  4.43988988,\n",
       "           7.15377888, 11.61952315]]),\n",
       "  30: array([[64.74      , 48.27550372,  3.19090972, ...,  1.87722812,\n",
       "           8.74837237, 17.81855662],\n",
       "         [66.02      , 48.10650875,  1.38370259, ...,  3.62947854,\n",
       "           5.35288123, 15.54871477],\n",
       "         [33.22285714, 39.2694665 ,  3.32336501, ...,  3.13178782,\n",
       "           3.82672979, 11.28293423],\n",
       "         ...,\n",
       "         [19.89428571, 50.204056  ,  1.75761951, ...,  3.68655149,\n",
       "           7.08103283, 16.05333247],\n",
       "         [40.60857143, 44.02342119,  2.20698707, ...,  2.48096854,\n",
       "           7.00611061, 27.3208608 ],\n",
       "         [52.23142857, 54.33189606,  2.26869361, ...,  4.80427092,\n",
       "           7.69694346, 15.22764944]]),\n",
       "  31: array([[71.88      , 40.59238942,  8.84389033, ...,  1.45634325,\n",
       "           3.86473613, 21.86370587],\n",
       "         [52.64      , 47.98766787,  1.67528233, ...,  3.39448642,\n",
       "           5.10750094, 16.59517744],\n",
       "         [42.10285714, 37.15973414,  3.15009351, ...,  2.46037195,\n",
       "           3.15367654, 13.22083195],\n",
       "         ...,\n",
       "         [18.15428571, 46.33127589,  2.60725847, ...,  3.7275565 ,\n",
       "           6.37168386, 13.59442787],\n",
       "         [55.78857143, 42.51180642,  2.80117193, ...,  2.26357733,\n",
       "           6.43945393, 22.85059899],\n",
       "         [31.23142857, 50.77593449,  2.44334637, ...,  3.88859797,\n",
       "           6.58710918, 12.33665198]]),\n",
       "  32: array([[71.1       , 44.25338332,  2.88436793, ...,  2.01546342,\n",
       "           4.10769009, 21.77335481],\n",
       "         [64.94      , 41.52482271,  1.19878624, ...,  3.87771344,\n",
       "           5.64132225, 23.77334654],\n",
       "         [43.96285714, 35.54997165,  3.00801727, ...,  2.68991935,\n",
       "           3.51814158, 13.11447349],\n",
       "         ...,\n",
       "         [28.17428571, 40.80024842,  1.68912947, ...,  3.34284211,\n",
       "           6.35901643, 11.80212083],\n",
       "         [57.04857143, 38.52006517,  1.98878717, ...,  2.47210014,\n",
       "           6.0339308 , 14.60155513],\n",
       "         [39.15142857, 47.22871608,  3.17274347, ...,  3.68859963,\n",
       "           5.74805436, 20.02588469]]),\n",
       "  33: array([[56.04      , 45.87823974,  4.26285502, ...,  2.4964641 ,\n",
       "           4.8934817 , 23.55579714],\n",
       "         [61.58      , 46.95926181,  7.14070464, ...,  5.75694266,\n",
       "           8.19207683, 17.77402753],\n",
       "         [41.62285714, 41.18510348,  8.80196328, ...,  2.32002503,\n",
       "           3.44993417,  8.5719898 ],\n",
       "         ...,\n",
       "         [21.03428571, 48.16018646,  3.3915443 , ...,  5.96291923,\n",
       "          10.42254349, 13.72052565],\n",
       "         [39.64857143, 50.43864119,  3.96760037, ...,  5.24354381,\n",
       "          10.9990749 , 16.14176054],\n",
       "         [46.95142857, 49.90703119,  3.2766322 , ..., 11.82908561,\n",
       "          11.98117291, 14.56110742]]),\n",
       "  34: array([[64.56      , 40.99323942,  2.87201828, ...,  1.5474276 ,\n",
       "           4.74023753, 21.45091094],\n",
       "         [63.08      , 45.87307053,  1.1620742 , ...,  2.83966578,\n",
       "           4.22689349, 12.97281992],\n",
       "         [38.32285714, 34.03251791,  2.63494315, ...,  2.05042055,\n",
       "           2.63078197, 14.19067074],\n",
       "         ...,\n",
       "         [32.25428571, 48.59972543,  1.57331279, ...,  2.94035616,\n",
       "           5.84983423,  9.23677541],\n",
       "         [47.38857143, 46.76988486,  2.0192168 , ...,  2.16818611,\n",
       "           7.19459824, 12.9196268 ],\n",
       "         [46.29142857, 47.09029492,  1.90315682, ...,  3.48851358,\n",
       "           5.85977645, 16.84670968]]),\n",
       "  35: array([[52.68      , 39.42745385,  2.85846137, ...,  1.41917656,\n",
       "           4.78460829, 19.96355588],\n",
       "         [64.94      , 50.15668987,  1.14745255, ...,  2.79534596,\n",
       "           6.19731439, 13.94353641],\n",
       "         [41.02285714, 33.82497428,  2.76508016, ...,  2.02197227,\n",
       "           2.85484012, 11.96687853],\n",
       "         ...,\n",
       "         [22.71428571, 44.66358173,  1.6767311 , ...,  2.77147556,\n",
       "           6.70700726, 15.9621959 ],\n",
       "         [44.92857143, 36.34988486,  1.98878717, ...,  2.05210014,\n",
       "           6.9609308 , 26.17855513],\n",
       "         [30.27142857, 50.44596074,  1.82134812, ...,  3.36491358,\n",
       "           8.05484751, 11.41322775]]),\n",
       "  36: array([[74.4       , 42.00915792,  2.77366927, ...,  1.49094806,\n",
       "           3.73481562, 16.95693819],\n",
       "         [62.84      , 44.89291276,  1.27261894, ...,  3.11327711,\n",
       "           4.43109691, 14.35934445],\n",
       "         [44.38285714, 38.36833941,  2.51696749, ...,  2.20917114,\n",
       "           2.45714818,  8.59078319],\n",
       "         ...,\n",
       "         [29.43428571, 46.37340955,  1.442341  , ...,  2.95140987,\n",
       "           5.53264906, 10.14081867],\n",
       "         [42.70857143, 45.06960691,  3.62110369, ...,  2.07912535,\n",
       "           6.09534649, 14.82475415],\n",
       "         [51.21142857, 46.61994864,  1.89469014, ...,  3.69134649,\n",
       "           5.49981779, 11.67397016]]),\n",
       "  37: array([[68.28      , 45.9082072 ,  2.77383674, ...,  1.35087349,\n",
       "           4.51571466, 23.01965264],\n",
       "         [66.02      , 44.09852522,  1.1603023 , ...,  2.78668112,\n",
       "           4.27721437, 22.98865112],\n",
       "         [31.54285714, 47.5714973 ,  2.84555505, ...,  2.39708516,\n",
       "           5.23934883,  8.99144619],\n",
       "         ...,\n",
       "         [24.69428571, 45.11926323,  1.79641055, ...,  3.27955842,\n",
       "           9.04870869, 11.63561279],\n",
       "         [44.02857143, 47.50178924,  2.17450803, ...,  2.37174477,\n",
       "          12.06695828, 13.33617592],\n",
       "         [36.87142857, 49.3603964 ,  2.04294417, ...,  3.6602786 ,\n",
       "           8.60388952, 16.35934872]]),\n",
       "  38: array([[68.16      , 40.8907974 ,  2.77363465, ...,  1.49711822,\n",
       "           3.8039136 , 18.52347758],\n",
       "         [61.82      , 49.66253702,  1.90432434, ...,  2.83642288,\n",
       "           4.84141796, 13.76849496],\n",
       "         [50.26285714, 33.16479959,  2.44714143, ...,  2.03572227,\n",
       "           4.68173025,  8.61347247],\n",
       "         ...,\n",
       "         [22.71428571, 45.67353921,  2.80023113, ...,  3.5294902 ,\n",
       "           6.18513805, 10.40168893],\n",
       "         [52.30857143, 44.24413142,  2.24011415, ...,  2.32322361,\n",
       "           6.68852132, 13.06406702],\n",
       "         [38.79142857, 49.67089287,  1.88219519, ...,  3.50112098,\n",
       "           5.64460967, 11.23796459]]),\n",
       "  39: array([[64.08      , 47.60122336,  9.44725138, ...,  1.95231857,\n",
       "           4.32470607, 20.50641079],\n",
       "         [58.34      , 47.17515899,  2.92072911, ...,  3.47599503,\n",
       "           5.29385933, 15.51759265],\n",
       "         [30.10285714, 37.10378638,  5.46997682, ...,  3.1581693 ,\n",
       "           2.44793872,  7.97820855],\n",
       "         ...,\n",
       "         [27.87428571, 46.47066211,  3.15137999, ...,  9.40664849,\n",
       "           7.14589528, 11.53343779],\n",
       "         [48.88857143, 49.21988486, 14.72825193, ...,  2.38181194,\n",
       "           7.12486246, 17.75660193],\n",
       "         [35.85142857, 51.33078452,  4.12096042, ...,  4.37738812,\n",
       "           7.41340154, 13.63980587]]),\n",
       "  40: array([[70.92      , 54.12744641,  3.95989489, ...,  3.1025752 ,\n",
       "           7.4742795 , 23.26975637],\n",
       "         [61.28      , 48.60248135,  1.42925781, ...,  7.05191968,\n",
       "           8.20188319, 21.9863678 ],\n",
       "         [27.58285714, 38.80593087,  3.86254776, ...,  2.57844708,\n",
       "           3.62749017, 12.47883311],\n",
       "         ...,\n",
       "         [38.31428571, 46.97188951,  2.4901983 , ...,  4.7848219 ,\n",
       "           8.23097427, 13.34046061],\n",
       "         [47.26857143, 50.47427732,  2.13020116, ...,  5.43785985,\n",
       "          16.24860498, 19.04241546],\n",
       "         [53.43142857, 58.79911961,  3.51442221, ...,  6.06350347,\n",
       "           8.9364176 , 15.73660843]]),\n",
       "  41: array([[53.04      , 45.8452318 ,  4.09457832, ...,  2.55563975,\n",
       "           5.432052  , 17.4151299 ],\n",
       "         [70.16      , 48.83775059,  1.83108192, ...,  6.51765372,\n",
       "           5.927246  , 16.53750494],\n",
       "         [29.44285714, 34.64674292,  3.50502835, ...,  2.99670372,\n",
       "           3.95358494,  9.76230456],\n",
       "         ...,\n",
       "         [25.65428571, 49.23404048,  2.36025524, ...,  3.94343183,\n",
       "           7.52506483, 11.11053051],\n",
       "         [38.02857143, 42.48986434,  2.94907522, ...,  3.07903649,\n",
       "           8.23845543, 14.48825416],\n",
       "         [41.25142857, 51.82054517,  2.60260243, ...,  5.03153019,\n",
       "           8.52940683, 11.70142911]]),\n",
       "  42: array([[57.12      , 44.06554873,  3.34397793, ...,  1.48005404,\n",
       "           4.43497193, 25.03690098],\n",
       "         [62.72      , 43.54195516,  1.53025853, ...,  2.98039389,\n",
       "           6.21110485, 24.02637999],\n",
       "         [29.26285714, 35.24347526,  3.04412822, ...,  2.23311243,\n",
       "           2.99852694, 13.51943714],\n",
       "         ...,\n",
       "         [19.77428571, 45.71654482,  1.87790391, ...,  3.15527065,\n",
       "           6.82184225, 15.78132616],\n",
       "         [45.76857143, 46.92210761,  2.20744301, ...,  2.40743743,\n",
       "          11.37351464, 22.27834625],\n",
       "         [37.05142857, 48.87166412,  2.21829572, ...,  3.58544873,\n",
       "           7.21173962, 20.83262277]]),\n",
       "  43: array([[70.26      , 40.58569583,  2.88203727, ...,  1.59628396,\n",
       "           3.93581046, 19.10354387],\n",
       "         [63.74      , 45.66583357,  1.22761944, ...,  2.96743548,\n",
       "           5.50466314, 15.80438469],\n",
       "         [37.90285714, 31.46704394,  3.35246107, ...,  2.37088081,\n",
       "           3.11125553,  8.22517173],\n",
       "         ...,\n",
       "         [33.15428571, 41.22487056,  1.51410873, ...,  3.23932527,\n",
       "           6.3608993 , 14.75115086],\n",
       "         [53.08857143, 39.62048768,  2.13699455, ...,  2.5863834 ,\n",
       "           7.24753437, 13.26644175],\n",
       "         [49.17142857, 46.93454559,  1.92996033, ...,  4.49658934,\n",
       "           6.82398829, 11.8601406 ]]),\n",
       "  44: array([[61.86      , 48.04885926,  3.06701037, ...,  1.72554976,\n",
       "           4.20352587, 16.5288448 ],\n",
       "         [61.58      , 49.62953198,  1.18157698, ...,  2.85875521,\n",
       "          10.83581954, 12.9688078 ],\n",
       "         [29.92285714, 38.78317404,  2.52539143, ...,  2.13972227,\n",
       "           2.74923025,  8.09997247],\n",
       "         ...,\n",
       "         [32.01428571, 49.82205307,  1.4127311 , ...,  4.32056646,\n",
       "           8.77376303,  8.64032025],\n",
       "         [41.20857143, 47.08488486,  1.98878717, ...,  3.61210014,\n",
       "          15.2139308 , 11.06155513],\n",
       "         [46.59142857, 52.50289824,  1.82668146, ...,  3.99005247,\n",
       "          19.24016455, 11.31631547]]),\n",
       "  45: array([[56.88      , 43.99545235,  3.04563091, ...,  1.50940608,\n",
       "           4.07755625, 20.44936729],\n",
       "         [62.        , 49.77660899,  1.5223258 , ...,  3.27726663,\n",
       "           5.36763461, 18.9157843 ],\n",
       "         [36.40285714, 35.84494997,  2.85719416, ...,  2.64274506,\n",
       "           3.13920612,  9.39565307],\n",
       "         ...,\n",
       "         [19.59428571, 51.64007195,  1.64800178, ...,  3.15786847,\n",
       "          11.38988193, 10.09704703],\n",
       "         [43.06857143, 44.25236046,  2.17247662, ...,  2.40903299,\n",
       "           6.60775823, 13.46502268],\n",
       "         [39.09142857, 52.11204383,  2.29217529, ...,  4.66887016,\n",
       "           6.76266991, 12.29202777]]),\n",
       "  46: array([[59.4       , 45.23329198,  3.24086098, ...,  2.1044778 ,\n",
       "           4.42700191, 17.91050526],\n",
       "         [59.12      , 47.61475715,  1.67734489, ...,  3.83992505,\n",
       "           5.31756183, 13.79402567],\n",
       "         [35.80285714, 40.74506449,  4.55476806, ...,  2.59317258,\n",
       "           3.86238628,  9.05821973],\n",
       "         ...,\n",
       "         [18.15428571, 47.91055705,  1.88908358, ...,  3.78944535,\n",
       "           6.74901947, 10.97043222],\n",
       "         [35.86857143, 43.92728859,  2.98997288, ...,  2.89347993,\n",
       "           9.1564646 , 12.73194782],\n",
       "         [28.89142857, 51.33555385,  2.46637648, ...,  4.61893916,\n",
       "           7.59852796, 12.72780074]]),\n",
       "  47: array([[67.44      , 45.15715058,  3.08825959, ...,  1.53794693,\n",
       "           4.0462799 , 18.71719557],\n",
       "         [73.52      , 46.0225074 ,  1.53012568, ...,  3.21303591,\n",
       "           4.47386702, 14.75768773],\n",
       "         [27.28285714, 38.62673149,  3.36933176, ...,  2.35237447,\n",
       "           3.41000937,  8.32469504],\n",
       "         ...,\n",
       "         [29.73428571, 48.72453737,  1.45991421, ...,  3.42017676,\n",
       "           7.2854219 ,  9.46821897],\n",
       "         [40.78857143, 48.04563355,  2.00740255, ...,  2.32040783,\n",
       "           6.09916157, 11.14032437],\n",
       "         [54.03142857, 48.54799039,  2.32303342, ...,  8.8278785 ,\n",
       "           6.08533314, 11.93704406]]),\n",
       "  48: array([[62.28      , 42.63941167,  2.80420494, ...,  1.53481117,\n",
       "           4.04391189, 16.85932383],\n",
       "         [66.14      , 44.81713754,  1.27818317, ...,  3.12403445,\n",
       "           6.75367149, 18.04810143],\n",
       "         [38.74285714, 42.91701893,  2.44017714, ...,  2.12547306,\n",
       "           2.41485427,  8.20982451],\n",
       "         ...,\n",
       "         [30.39428571, 52.07996728,  1.51311397, ...,  3.20749456,\n",
       "           9.19506931,  9.78446248],\n",
       "         [54.58857143, 45.42407851,  1.98878717, ...,  2.05210014,\n",
       "          18.1839308 , 11.30155513],\n",
       "         [53.55142857, 54.26947235,  2.18042859, ...,  4.06463492,\n",
       "           6.12445197, 11.03961005]]),\n",
       "  49: array([[60.06      , 45.14043114,  2.8173535 , ...,  1.36770903,\n",
       "           3.86308607, 19.09703011],\n",
       "         [63.38      , 47.11765054,  1.26683   , ...,  3.016735  ,\n",
       "           4.63369491, 21.32481118],\n",
       "         [36.04285714, 34.65512457,  2.70232583, ...,  2.20902414,\n",
       "           2.71666401,  9.18300813],\n",
       "         ...,\n",
       "         [25.35428571, 47.35605019,  1.51522312, ...,  2.9979482 ,\n",
       "           6.34899749, 10.67453445],\n",
       "         [45.40857143, 43.7377999 ,  2.04516959, ...,  2.17459738,\n",
       "           6.40468165, 17.02454873],\n",
       "         [40.29142857, 49.51951361,  1.89065285, ...,  3.51016087,\n",
       "           6.13667962, 11.90884723]])}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subject'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfcs_leaderboard_predictionresult = pd.DataFrame(data = _)\n",
    "df_rfcs_leaderboard_predictionresult.to_csv('결과1/rfcs_leaderboard_predictionresult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/kimjaehee/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_forest.py:815: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For subchallenge 1:\n",
      "\tScore = 28.05\n",
      "\tint = 0.886\n",
      "\tple = 0.150\n",
      "\tdec = 0.119\n"
     ]
    }
   ],
   "source": [
    "rfcs,score,rs = fit1.rfc_final(X_all,Y_all_imp['subject'],\n",
    "                            max_features,min_samples_leaf,max_depth,et,\n",
    "                            n_estimators=n_estimators)\n",
    "\n",
    "df_rfcs = pd.DataFrame(data = rfcs)\n",
    "df_rfcs.to_csv('결과1/rfcs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17.51142857 45.45103048  1.82887743 ...  3.16909256  3.13712229\n",
      "  14.25814411]\n",
      " [22.73714286 41.30419574  2.14069816 ...  2.24551212  3.56814451\n",
      "  17.038831  ]\n",
      " [49.44       39.74036856  1.85292398 ...  1.73408968  3.57986147\n",
      "   8.61641725]\n",
      " ...\n",
      " [54.96857143 45.8025428   2.57607782 ...  4.6392501   5.7437693\n",
      "   9.3322232 ]\n",
      " [23.56285714 49.36120199  2.8558064  ...  3.45734144  5.51156672\n",
      "   9.97267811]\n",
      " [48.05714286 45.46695469  1.93624672 ...  3.13132904 12.1112818\n",
      "  13.70763939]]\n"
     ]
    }
   ],
   "source": [
    "_ = loading.make_prediction_files(rfcs,X_testset_int,X_testset_other,'testset',1,write=False)\n",
    "print(_['subject'][1]) # Print values for subject 1 to make sure it looks right. \n",
    "\n",
    "df_rfcs_predictionresult = pd.DataFrame(data = _)\n",
    "df_rfcs_predictionresult.to_csv('결과1/rfcs_predictionresult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('py36': conda)",
   "language": "python",
   "name": "python361064bitpy36condafebe6ef44cbe421b8ee9be5fd6c2df2e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
